{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework-3: MLP for MNIST Classification\n",
    "\n",
    "### In this homework, you need to\n",
    "- #### implement SGD optimizer (`./optimizer.py`)\n",
    "- #### implement forward and backward for FCLayer (`layers/fc_layer.py`)\n",
    "- #### implement forward and backward for SigmoidLayer (`layers/sigmoid_layer.py`)\n",
    "- #### implement forward and backward for ReLULayer (`layers/relu_layer.py`)\n",
    "- #### implement EuclideanLossLayer (`criterion/euclidean_loss.py`)\n",
    "- #### implement SoftmaxCrossEntropyLossLayer (`criterion/softmax_cross_entropy.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T15:53:25.067125Z",
     "start_time": "2021-10-17T15:53:25.057366Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "from network import Network\n",
    "from solver import train, test\n",
    "from plot import plot_loss_and_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset\n",
    "We use tensorflow tools to load dataset for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T15:53:26.562599Z",
     "start_time": "2021-10-17T15:53:26.333801Z"
    }
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T15:53:27.044701Z",
     "start_time": "2021-10-17T15:53:27.040817Z"
    }
   },
   "outputs": [],
   "source": [
    "def decode_image(image):\n",
    "    # Normalize from [0, 255.] to [0., 1.0], and then subtract by the mean value\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.reshape(image, [784])\n",
    "    image = image / 255.0\n",
    "    image = image - tf.reduce_mean(image)\n",
    "    return image\n",
    "\n",
    "def decode_label(label):\n",
    "    # Encode label with one-hot encoding\n",
    "    return tf.one_hot(label, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T15:53:27.962120Z",
     "start_time": "2021-10-17T15:53:27.725289Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "x_train = tf.data.Dataset.from_tensor_slices(x_train).map(decode_image)\n",
    "y_train = tf.data.Dataset.from_tensor_slices(y_train).map(decode_label)\n",
    "data_train = tf.data.Dataset.zip((x_train, y_train))\n",
    "\n",
    "x_test = tf.data.Dataset.from_tensor_slices(x_test).map(decode_image)\n",
    "y_test = tf.data.Dataset.from_tensor_slices(y_test).map(decode_label)\n",
    "data_test = tf.data.Dataset.zip((x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T14:26:38.622727Z",
     "start_time": "2021-10-17T14:26:38.613325Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyerparameters\n",
    "You can modify hyerparameters by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T15:53:31.488091Z",
     "start_time": "2021-10-17T15:53:31.485172Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "max_epoch = 20\n",
    "init_std = 0.01\n",
    "\n",
    "learning_rate_SGD = 0.001\n",
    "weight_decay = 0.1\n",
    "\n",
    "disp_freq = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MLP with Euclidean Loss\n",
    "In part-1, you need to train a MLP with **Euclidean Loss**.  \n",
    "**Sigmoid Activation Function** and **ReLU Activation Function** will be used respectively.\n",
    "### TODO\n",
    "Before executing the following code, you should complete **./optimizer.py** and **criterion/euclidean_loss.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T15:55:45.537257Z",
     "start_time": "2021-10-17T15:55:45.530189Z"
    }
   },
   "outputs": [],
   "source": [
    "from criterion import EuclideanLossLayer\n",
    "from optimizer import SGD\n",
    "\n",
    "criterion = EuclideanLossLayer()\n",
    "\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 MLP with Euclidean Loss and Sigmoid Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using Sigmoid activation function and Euclidean loss function.\n",
    "\n",
    "### TODO\n",
    "Before executing the following code, you should complete **layers/fc_layer.py** and **layers/sigmoid_layer.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T15:55:47.095509Z",
     "start_time": "2021-10-17T15:55:47.085608Z"
    }
   },
   "outputs": [],
   "source": [
    "from layers import FCLayer, SigmoidLayer\n",
    "\n",
    "sigmoidMLP = Network()\n",
    "# Build MLP with FCLayer and SigmoidLayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "sigmoidMLP.add(FCLayer(784, 128))\n",
    "sigmoidMLP.add(SigmoidLayer())\n",
    "sigmoidMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T15:58:16.154998Z",
     "start_time": "2021-10-17T15:55:48.785493Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 3.3259\t Accuracy 0.1300\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 4.7014\t Accuracy 0.1345\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.5957\t Accuracy 0.2390\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 1.8650\t Accuracy 0.2963\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 1.4846\t Accuracy 0.3508\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 1.2496\t Accuracy 0.4096\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 1.0897\t Accuracy 0.4588\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.9740\t Accuracy 0.4977\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.8855\t Accuracy 0.5301\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.8158\t Accuracy 0.5564\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.7591\t Accuracy 0.5793\n",
      "\n",
      "Epoch [0]\t Average training loss 0.7127\t Average training accuracy 0.5977\n",
      "Epoch [0]\t Average validation loss 0.2204\t Average validation accuracy 0.8230\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.2080\t Accuracy 0.8300\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.2276\t Accuracy 0.8061\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.2261\t Accuracy 0.8095\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.2266\t Accuracy 0.8051\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.2241\t Accuracy 0.8084\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.2216\t Accuracy 0.8121\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.2196\t Accuracy 0.8145\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.2184\t Accuracy 0.8167\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.2163\t Accuracy 0.8206\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.2145\t Accuracy 0.8236\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.2127\t Accuracy 0.8261\n",
      "\n",
      "Epoch [1]\t Average training loss 0.2105\t Average training accuracy 0.8286\n",
      "Epoch [1]\t Average validation loss 0.1707\t Average validation accuracy 0.8874\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.1597\t Accuracy 0.9100\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.1813\t Accuracy 0.8680\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.1811\t Accuracy 0.8659\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.1832\t Accuracy 0.8605\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.1819\t Accuracy 0.8631\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.1805\t Accuracy 0.8643\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.1796\t Accuracy 0.8655\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.1793\t Accuracy 0.8653\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.1783\t Accuracy 0.8664\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.1774\t Accuracy 0.8671\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.1766\t Accuracy 0.8674\n",
      "\n",
      "Epoch [2]\t Average training loss 0.1753\t Average training accuracy 0.8686\n",
      "Epoch [2]\t Average validation loss 0.1440\t Average validation accuracy 0.9064\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.1340\t Accuracy 0.9300\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.1555\t Accuracy 0.8894\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.1561\t Accuracy 0.8847\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.1588\t Accuracy 0.8807\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.1578\t Accuracy 0.8835\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.1569\t Accuracy 0.8839\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.1562\t Accuracy 0.8846\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.1563\t Accuracy 0.8836\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.1557\t Accuracy 0.8841\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.1552\t Accuracy 0.8842\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.1548\t Accuracy 0.8839\n",
      "\n",
      "Epoch [3]\t Average training loss 0.1538\t Average training accuracy 0.8844\n",
      "Epoch [3]\t Average validation loss 0.1264\t Average validation accuracy 0.9154\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.1181\t Accuracy 0.9400\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.1378\t Accuracy 0.9002\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.1389\t Accuracy 0.8957\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.1418\t Accuracy 0.8915\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.1409\t Accuracy 0.8937\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.1403\t Accuracy 0.8940\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.1398\t Accuracy 0.8945\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.1401\t Accuracy 0.8933\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.1397\t Accuracy 0.8930\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.1395\t Accuracy 0.8932\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.1393\t Accuracy 0.8928\n",
      "\n",
      "Epoch [4]\t Average training loss 0.1386\t Average training accuracy 0.8930\n",
      "Epoch [4]\t Average validation loss 0.1142\t Average validation accuracy 0.9212\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.1074\t Accuracy 0.9400\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.1252\t Accuracy 0.9057\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.1267\t Accuracy 0.9021\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.1297\t Accuracy 0.8983\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.1290\t Accuracy 0.9000\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.1286\t Accuracy 0.9004\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.1282\t Accuracy 0.9008\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.1286\t Accuracy 0.8995\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.1284\t Accuracy 0.8993\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.1283\t Accuracy 0.8995\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.1283\t Accuracy 0.8990\n",
      "\n",
      "Epoch [5]\t Average training loss 0.1278\t Average training accuracy 0.8990\n",
      "Epoch [5]\t Average validation loss 0.1058\t Average validation accuracy 0.9256\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.1001\t Accuracy 0.9400\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.1163\t Accuracy 0.9118\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.1179\t Accuracy 0.9067\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.1210\t Accuracy 0.9030\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.1204\t Accuracy 0.9047\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.1201\t Accuracy 0.9052\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.1199\t Accuracy 0.9053\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.1203\t Accuracy 0.9040\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.1202\t Accuracy 0.9038\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.1203\t Accuracy 0.9040\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.1204\t Accuracy 0.9034\n",
      "\n",
      "Epoch [6]\t Average training loss 0.1200\t Average training accuracy 0.9035\n",
      "Epoch [6]\t Average validation loss 0.0997\t Average validation accuracy 0.9288\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.0951\t Accuracy 0.9500\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.1098\t Accuracy 0.9147\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.1116\t Accuracy 0.9104\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.1147\t Accuracy 0.9066\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.1141\t Accuracy 0.9078\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.1139\t Accuracy 0.9081\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.1138\t Accuracy 0.9084\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.1143\t Accuracy 0.9072\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.1142\t Accuracy 0.9071\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.1143\t Accuracy 0.9071\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.1146\t Accuracy 0.9067\n",
      "\n",
      "Epoch [7]\t Average training loss 0.1142\t Average training accuracy 0.9068\n",
      "Epoch [7]\t Average validation loss 0.0953\t Average validation accuracy 0.9304\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.0916\t Accuracy 0.9500\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.1051\t Accuracy 0.9178\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.1068\t Accuracy 0.9137\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.1099\t Accuracy 0.9101\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.1094\t Accuracy 0.9110\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.1093\t Accuracy 0.9114\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.1092\t Accuracy 0.9115\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.1097\t Accuracy 0.9103\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.1097\t Accuracy 0.9103\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.1098\t Accuracy 0.9101\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.1101\t Accuracy 0.9095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8]\t Average training loss 0.1099\t Average training accuracy 0.9095\n",
      "Epoch [8]\t Average validation loss 0.0919\t Average validation accuracy 0.9330\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0891\t Accuracy 0.9500\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.1014\t Accuracy 0.9204\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.1032\t Accuracy 0.9167\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.1063\t Accuracy 0.9128\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.1058\t Accuracy 0.9137\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.1057\t Accuracy 0.9139\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.1056\t Accuracy 0.9140\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.1062\t Accuracy 0.9129\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.1063\t Accuracy 0.9127\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.1064\t Accuracy 0.9126\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.1067\t Accuracy 0.9119\n",
      "\n",
      "Epoch [9]\t Average training loss 0.1065\t Average training accuracy 0.9118\n",
      "Epoch [9]\t Average validation loss 0.0893\t Average validation accuracy 0.9342\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0872\t Accuracy 0.9500\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0986\t Accuracy 0.9222\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.1004\t Accuracy 0.9185\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.1034\t Accuracy 0.9146\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.1029\t Accuracy 0.9154\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.1029\t Accuracy 0.9155\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.1029\t Accuracy 0.9155\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.1034\t Accuracy 0.9145\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.1035\t Accuracy 0.9143\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.1037\t Accuracy 0.9143\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.1041\t Accuracy 0.9136\n",
      "\n",
      "Epoch [10]\t Average training loss 0.1038\t Average training accuracy 0.9135\n",
      "Epoch [10]\t Average validation loss 0.0873\t Average validation accuracy 0.9352\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0857\t Accuracy 0.9500\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0963\t Accuracy 0.9235\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0982\t Accuracy 0.9197\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.1011\t Accuracy 0.9156\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.1006\t Accuracy 0.9167\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.1006\t Accuracy 0.9169\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.1006\t Accuracy 0.9171\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.1012\t Accuracy 0.9160\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.1013\t Accuracy 0.9159\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.1015\t Accuracy 0.9158\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.1019\t Accuracy 0.9150\n",
      "\n",
      "Epoch [11]\t Average training loss 0.1017\t Average training accuracy 0.9149\n",
      "Epoch [11]\t Average validation loss 0.0856\t Average validation accuracy 0.9354\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0846\t Accuracy 0.9500\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0945\t Accuracy 0.9249\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0963\t Accuracy 0.9205\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0993\t Accuracy 0.9164\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0988\t Accuracy 0.9176\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0988\t Accuracy 0.9178\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0988\t Accuracy 0.9178\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0994\t Accuracy 0.9168\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0995\t Accuracy 0.9169\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0997\t Accuracy 0.9169\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.1001\t Accuracy 0.9160\n",
      "\n",
      "Epoch [12]\t Average training loss 0.1000\t Average training accuracy 0.9159\n",
      "Epoch [12]\t Average validation loss 0.0843\t Average validation accuracy 0.9362\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0837\t Accuracy 0.9500\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0929\t Accuracy 0.9255\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0948\t Accuracy 0.9218\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0977\t Accuracy 0.9177\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0973\t Accuracy 0.9189\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0973\t Accuracy 0.9188\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0973\t Accuracy 0.9187\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0979\t Accuracy 0.9179\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0981\t Accuracy 0.9179\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0982\t Accuracy 0.9180\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0987\t Accuracy 0.9171\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0985\t Average training accuracy 0.9171\n",
      "Epoch [13]\t Average validation loss 0.0831\t Average validation accuracy 0.9364\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0830\t Accuracy 0.9500\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0917\t Accuracy 0.9257\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0935\t Accuracy 0.9223\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0964\t Accuracy 0.9184\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0960\t Accuracy 0.9194\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0960\t Accuracy 0.9194\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0961\t Accuracy 0.9192\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0967\t Accuracy 0.9183\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0968\t Accuracy 0.9183\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0970\t Accuracy 0.9184\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0975\t Accuracy 0.9176\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0973\t Average training accuracy 0.9175\n",
      "Epoch [14]\t Average validation loss 0.0822\t Average validation accuracy 0.9368\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0823\t Accuracy 0.9500\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0906\t Accuracy 0.9267\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0924\t Accuracy 0.9234\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0953\t Accuracy 0.9193\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0949\t Accuracy 0.9203\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0949\t Accuracy 0.9202\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0950\t Accuracy 0.9200\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0956\t Accuracy 0.9191\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0958\t Accuracy 0.9190\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0960\t Accuracy 0.9191\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0964\t Accuracy 0.9182\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0963\t Average training accuracy 0.9181\n",
      "Epoch [15]\t Average validation loss 0.0814\t Average validation accuracy 0.9374\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0818\t Accuracy 0.9500\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0896\t Accuracy 0.9265\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0915\t Accuracy 0.9236\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0944\t Accuracy 0.9193\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0939\t Accuracy 0.9203\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0940\t Accuracy 0.9204\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0941\t Accuracy 0.9203\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0947\t Accuracy 0.9194\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0949\t Accuracy 0.9194\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0951\t Accuracy 0.9194\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0955\t Accuracy 0.9185\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0954\t Average training accuracy 0.9185\n",
      "Epoch [16]\t Average validation loss 0.0807\t Average validation accuracy 0.9378\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0813\t Accuracy 0.9500\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0888\t Accuracy 0.9278\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0907\t Accuracy 0.9242\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0935\t Accuracy 0.9200\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0931\t Accuracy 0.9211\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0932\t Accuracy 0.9212\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0933\t Accuracy 0.9210\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0939\t Accuracy 0.9201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0941\t Accuracy 0.9200\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0943\t Accuracy 0.9200\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0947\t Accuracy 0.9191\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0946\t Average training accuracy 0.9191\n",
      "Epoch [17]\t Average validation loss 0.0802\t Average validation accuracy 0.9384\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0809\t Accuracy 0.9500\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0881\t Accuracy 0.9284\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0900\t Accuracy 0.9246\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0928\t Accuracy 0.9205\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0924\t Accuracy 0.9217\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0925\t Accuracy 0.9218\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0926\t Accuracy 0.9216\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0932\t Accuracy 0.9206\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0934\t Accuracy 0.9205\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0936\t Accuracy 0.9205\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0941\t Accuracy 0.9196\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0939\t Average training accuracy 0.9195\n",
      "Epoch [18]\t Average validation loss 0.0797\t Average validation accuracy 0.9386\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0806\t Accuracy 0.9500\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0875\t Accuracy 0.9286\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0893\t Accuracy 0.9250\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0922\t Accuracy 0.9209\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0918\t Accuracy 0.9220\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0918\t Accuracy 0.9221\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0920\t Accuracy 0.9220\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0926\t Accuracy 0.9211\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0928\t Accuracy 0.9209\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0930\t Accuracy 0.9209\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0935\t Accuracy 0.9200\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0934\t Average training accuracy 0.9199\n",
      "Epoch [19]\t Average validation loss 0.0793\t Average validation accuracy 0.9398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T15:59:04.503737Z",
     "start_time": "2021-10-17T15:58:32.373357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9263.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 MLP with Euclidean Loss and ReLU Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using ReLU activation function and Euclidean loss function.\n",
    "\n",
    "### TODO\n",
    "Before executing the following code, you should complete **layers/relu_layer.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T15:59:04.526239Z",
     "start_time": "2021-10-17T15:59:04.508021Z"
    }
   },
   "outputs": [],
   "source": [
    "from layers import ReLULayer\n",
    "\n",
    "reluMLP = Network()\n",
    "# TODO build ReLUMLP with FCLayer and ReLULayer\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T16:01:59.946465Z",
     "start_time": "2021-10-17T15:59:04.529768Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 4.2168\t Accuracy 0.1900\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.6317\t Accuracy 0.5265\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.4476\t Accuracy 0.6481\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.3742\t Accuracy 0.7031\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.3302\t Accuracy 0.7371\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.2995\t Accuracy 0.7626\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.2772\t Accuracy 0.7807\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.2601\t Accuracy 0.7937\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.2460\t Accuracy 0.8055\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.2345\t Accuracy 0.8151\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.2249\t Accuracy 0.8222\n",
      "\n",
      "Epoch [0]\t Average training loss 0.2161\t Average training accuracy 0.8295\n",
      "Epoch [0]\t Average validation loss 0.1102\t Average validation accuracy 0.9262\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.1108\t Accuracy 0.9500\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.1194\t Accuracy 0.9112\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.1188\t Accuracy 0.9105\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.1202\t Accuracy 0.9076\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.1189\t Accuracy 0.9094\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.1177\t Accuracy 0.9102\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.1170\t Accuracy 0.9107\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.1163\t Accuracy 0.9110\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.1155\t Accuracy 0.9115\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.1147\t Accuracy 0.9122\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.1142\t Accuracy 0.9124\n",
      "\n",
      "Epoch [1]\t Average training loss 0.1130\t Average training accuracy 0.9138\n",
      "Epoch [1]\t Average validation loss 0.0897\t Average validation accuracy 0.9462\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.0910\t Accuracy 0.9500\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.0969\t Accuracy 0.9296\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.0977\t Accuracy 0.9271\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.0997\t Accuracy 0.9244\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.0989\t Accuracy 0.9260\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.0983\t Accuracy 0.9265\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.0982\t Accuracy 0.9261\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.0981\t Accuracy 0.9264\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.0979\t Accuracy 0.9270\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.0975\t Accuracy 0.9270\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.0976\t Accuracy 0.9265\n",
      "\n",
      "Epoch [2]\t Average training loss 0.0968\t Average training accuracy 0.9274\n",
      "Epoch [2]\t Average validation loss 0.0806\t Average validation accuracy 0.9512\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.0828\t Accuracy 0.9500\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.0864\t Accuracy 0.9402\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.0876\t Accuracy 0.9377\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.0895\t Accuracy 0.9348\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.0888\t Accuracy 0.9356\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.0884\t Accuracy 0.9355\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.0885\t Accuracy 0.9350\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.0885\t Accuracy 0.9348\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.0884\t Accuracy 0.9352\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.0882\t Accuracy 0.9354\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.0885\t Accuracy 0.9347\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0879\t Average training accuracy 0.9355\n",
      "Epoch [3]\t Average validation loss 0.0749\t Average validation accuracy 0.9558\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.0769\t Accuracy 0.9700\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.0793\t Accuracy 0.9484\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.0809\t Accuracy 0.9450\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.0826\t Accuracy 0.9424\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.0820\t Accuracy 0.9426\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.0816\t Accuracy 0.9421\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.0818\t Accuracy 0.9415\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.0819\t Accuracy 0.9411\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.0819\t Accuracy 0.9414\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.0818\t Accuracy 0.9416\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.0821\t Accuracy 0.9408\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0816\t Average training accuracy 0.9416\n",
      "Epoch [4]\t Average validation loss 0.0706\t Average validation accuracy 0.9596\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.0721\t Accuracy 0.9700\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.0742\t Accuracy 0.9494\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.0759\t Accuracy 0.9475\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.0775\t Accuracy 0.9463\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.0769\t Accuracy 0.9467\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.0765\t Accuracy 0.9463\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.0768\t Accuracy 0.9457\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.0770\t Accuracy 0.9456\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.0770\t Accuracy 0.9460\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.0769\t Accuracy 0.9461\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.0773\t Accuracy 0.9454\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0768\t Average training accuracy 0.9460\n",
      "Epoch [5]\t Average validation loss 0.0672\t Average validation accuracy 0.9618\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.0686\t Accuracy 0.9700\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.0702\t Accuracy 0.9551\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.0720\t Accuracy 0.9523\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.0734\t Accuracy 0.9507\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.0730\t Accuracy 0.9512\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.0725\t Accuracy 0.9507\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.0729\t Accuracy 0.9502\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.0730\t Accuracy 0.9499\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.0731\t Accuracy 0.9501\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.0730\t Accuracy 0.9500\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.0734\t Accuracy 0.9494\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0730\t Average training accuracy 0.9498\n",
      "Epoch [6]\t Average validation loss 0.0644\t Average validation accuracy 0.9638\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.0655\t Accuracy 0.9700\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.0669\t Accuracy 0.9580\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.0688\t Accuracy 0.9557\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.0701\t Accuracy 0.9546\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.0697\t Accuracy 0.9546\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.0692\t Accuracy 0.9546\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.0697\t Accuracy 0.9541\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.0698\t Accuracy 0.9537\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.0699\t Accuracy 0.9537\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.0698\t Accuracy 0.9536\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.0703\t Accuracy 0.9527\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0699\t Average training accuracy 0.9530\n",
      "Epoch [7]\t Average validation loss 0.0622\t Average validation accuracy 0.9678\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.0634\t Accuracy 0.9700\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.0643\t Accuracy 0.9608\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.0662\t Accuracy 0.9580\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.0674\t Accuracy 0.9574\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.0670\t Accuracy 0.9569\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.0666\t Accuracy 0.9569\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.0670\t Accuracy 0.9563\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.0672\t Accuracy 0.9558\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.0673\t Accuracy 0.9559\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.0672\t Accuracy 0.9559\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.0677\t Accuracy 0.9551\n",
      "\n",
      "Epoch [8]\t Average training loss 0.0673\t Average training accuracy 0.9554\n",
      "Epoch [8]\t Average validation loss 0.0605\t Average validation accuracy 0.9692\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0615\t Accuracy 0.9700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.0621\t Accuracy 0.9635\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.0639\t Accuracy 0.9605\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.0650\t Accuracy 0.9600\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.0647\t Accuracy 0.9596\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.0643\t Accuracy 0.9595\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.0648\t Accuracy 0.9587\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.0649\t Accuracy 0.9582\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.0650\t Accuracy 0.9584\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.0650\t Accuracy 0.9582\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.0655\t Accuracy 0.9573\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0652\t Average training accuracy 0.9575\n",
      "Epoch [9]\t Average validation loss 0.0589\t Average validation accuracy 0.9698\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0594\t Accuracy 0.9700\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0602\t Accuracy 0.9657\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.0619\t Accuracy 0.9620\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.0630\t Accuracy 0.9615\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.0627\t Accuracy 0.9611\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.0624\t Accuracy 0.9610\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.0628\t Accuracy 0.9604\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.0630\t Accuracy 0.9599\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.0631\t Accuracy 0.9600\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.0631\t Accuracy 0.9598\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.0635\t Accuracy 0.9589\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0633\t Average training accuracy 0.9591\n",
      "Epoch [10]\t Average validation loss 0.0577\t Average validation accuracy 0.9698\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0579\t Accuracy 0.9700\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0586\t Accuracy 0.9673\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0603\t Accuracy 0.9631\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.0613\t Accuracy 0.9627\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.0610\t Accuracy 0.9627\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.0607\t Accuracy 0.9625\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.0612\t Accuracy 0.9617\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.0613\t Accuracy 0.9614\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.0615\t Accuracy 0.9614\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.0614\t Accuracy 0.9612\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.0619\t Accuracy 0.9605\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0617\t Average training accuracy 0.9607\n",
      "Epoch [11]\t Average validation loss 0.0565\t Average validation accuracy 0.9702\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0564\t Accuracy 0.9700\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0571\t Accuracy 0.9682\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0587\t Accuracy 0.9642\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0598\t Accuracy 0.9638\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0595\t Accuracy 0.9638\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0592\t Accuracy 0.9635\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0597\t Accuracy 0.9627\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0599\t Accuracy 0.9626\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0600\t Accuracy 0.9626\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0600\t Accuracy 0.9624\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.0605\t Accuracy 0.9617\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0602\t Average training accuracy 0.9619\n",
      "Epoch [12]\t Average validation loss 0.0556\t Average validation accuracy 0.9714\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0552\t Accuracy 0.9700\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0558\t Accuracy 0.9678\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0574\t Accuracy 0.9650\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0584\t Accuracy 0.9648\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0582\t Accuracy 0.9648\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0579\t Accuracy 0.9646\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0584\t Accuracy 0.9638\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0585\t Accuracy 0.9638\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0587\t Accuracy 0.9638\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0587\t Accuracy 0.9636\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0592\t Accuracy 0.9628\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0590\t Average training accuracy 0.9629\n",
      "Epoch [13]\t Average validation loss 0.0547\t Average validation accuracy 0.9708\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0541\t Accuracy 0.9700\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0546\t Accuracy 0.9700\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0562\t Accuracy 0.9666\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0572\t Accuracy 0.9662\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0570\t Accuracy 0.9660\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0567\t Accuracy 0.9659\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0573\t Accuracy 0.9649\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0574\t Accuracy 0.9650\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0575\t Accuracy 0.9649\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0575\t Accuracy 0.9647\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0580\t Accuracy 0.9639\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0578\t Average training accuracy 0.9640\n",
      "Epoch [14]\t Average validation loss 0.0540\t Average validation accuracy 0.9712\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0532\t Accuracy 0.9700\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0536\t Accuracy 0.9704\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0552\t Accuracy 0.9674\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0561\t Accuracy 0.9672\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0560\t Accuracy 0.9670\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0557\t Accuracy 0.9670\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0563\t Accuracy 0.9661\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0564\t Accuracy 0.9661\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0565\t Accuracy 0.9660\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0565\t Accuracy 0.9659\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0570\t Accuracy 0.9651\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0568\t Average training accuracy 0.9652\n",
      "Epoch [15]\t Average validation loss 0.0534\t Average validation accuracy 0.9724\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0522\t Accuracy 0.9700\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0526\t Accuracy 0.9718\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0543\t Accuracy 0.9686\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0552\t Accuracy 0.9683\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0551\t Accuracy 0.9681\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0548\t Accuracy 0.9682\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0553\t Accuracy 0.9673\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0554\t Accuracy 0.9672\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0556\t Accuracy 0.9671\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0556\t Accuracy 0.9669\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0561\t Accuracy 0.9661\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0559\t Average training accuracy 0.9662\n",
      "Epoch [16]\t Average validation loss 0.0527\t Average validation accuracy 0.9726\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0516\t Accuracy 0.9700\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0518\t Accuracy 0.9716\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0534\t Accuracy 0.9691\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0543\t Accuracy 0.9691\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0542\t Accuracy 0.9689\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0539\t Accuracy 0.9688\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0545\t Accuracy 0.9679\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0546\t Accuracy 0.9679\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0547\t Accuracy 0.9679\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0547\t Accuracy 0.9675\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0552\t Accuracy 0.9668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [17]\t Average training loss 0.0551\t Average training accuracy 0.9669\n",
      "Epoch [17]\t Average validation loss 0.0522\t Average validation accuracy 0.9728\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0509\t Accuracy 0.9700\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0510\t Accuracy 0.9724\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0526\t Accuracy 0.9695\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0536\t Accuracy 0.9696\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0534\t Accuracy 0.9694\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0532\t Accuracy 0.9694\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0537\t Accuracy 0.9685\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0538\t Accuracy 0.9685\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0540\t Accuracy 0.9684\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0540\t Accuracy 0.9681\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0545\t Accuracy 0.9673\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0543\t Average training accuracy 0.9674\n",
      "Epoch [18]\t Average validation loss 0.0517\t Average validation accuracy 0.9730\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0505\t Accuracy 0.9700\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0503\t Accuracy 0.9725\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0519\t Accuracy 0.9699\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0528\t Accuracy 0.9702\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0527\t Accuracy 0.9700\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0524\t Accuracy 0.9701\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0530\t Accuracy 0.9692\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0531\t Accuracy 0.9690\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0533\t Accuracy 0.9690\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0533\t Accuracy 0.9686\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0538\t Accuracy 0.9678\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0536\t Average training accuracy 0.9679\n",
      "Epoch [19]\t Average validation loss 0.0513\t Average validation accuracy 0.9738\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T16:02:36.488938Z",
     "start_time": "2021-10-17T16:01:59.951185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9636.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T16:02:37.690053Z",
     "start_time": "2021-10-17T16:02:36.492733Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\n",
    "                   'relu': [relu_loss, relu_acc]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MLP with Softmax Cross-Entropy Loss\n",
    "In part-2, you need to train a MLP with **Softmax Cross-Entropy Loss**.  \n",
    "**Sigmoid Activation Function** and **ReLU Activation Function** will be used respectively again.\n",
    "### TODO\n",
    "Before executing the following code, you should complete **criterion/softmax_cross_entropy_loss.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T16:02:37.695047Z",
     "start_time": "2021-10-17T16:02:37.692018Z"
    }
   },
   "outputs": [],
   "source": [
    "from criterion import SoftmaxCrossEntropyLossLayer\n",
    "\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\n",
    "\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 MLP with Softmax Cross-Entropy Loss and Sigmoid Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using Sigmoid activation function and Softmax cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T16:02:37.710594Z",
     "start_time": "2021-10-17T16:02:37.697063Z"
    }
   },
   "outputs": [],
   "source": [
    "sigmoidMLP = Network()\n",
    "# Build MLP with FCLayer and SigmoidLayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "sigmoidMLP.add(FCLayer(784, 128))\n",
    "sigmoidMLP.add(SigmoidLayer())\n",
    "sigmoidMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T16:06:11.538790Z",
     "start_time": "2021-10-17T16:02:37.716356Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 3.2443\t Accuracy 0.0600\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 1.8360\t Accuracy 0.5524\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 1.5177\t Accuracy 0.6624\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 1.3284\t Accuracy 0.7091\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 1.1965\t Accuracy 0.7396\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 1.0942\t Accuracy 0.7611\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 1.0163\t Accuracy 0.7778\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.9600\t Accuracy 0.7887\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.9086\t Accuracy 0.7986\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.8669\t Accuracy 0.8061\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.8318\t Accuracy 0.8120\n",
      "\n",
      "Epoch [0]\t Average training loss 0.8009\t Average training accuracy 0.8175\n",
      "Epoch [0]\t Average validation loss 0.3918\t Average validation accuracy 0.9112\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.4584\t Accuracy 0.9000\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.4411\t Accuracy 0.8910\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.4429\t Accuracy 0.8885\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.4494\t Accuracy 0.8850\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.4447\t Accuracy 0.8863\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.4385\t Accuracy 0.8874\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.4345\t Accuracy 0.8885\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.4348\t Accuracy 0.8870\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.4307\t Accuracy 0.8875\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.4279\t Accuracy 0.8880\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.4260\t Accuracy 0.8879\n",
      "\n",
      "Epoch [1]\t Average training loss 0.4233\t Average training accuracy 0.8882\n",
      "Epoch [1]\t Average validation loss 0.3101\t Average validation accuracy 0.9240\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.3602\t Accuracy 0.9400\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.3552\t Accuracy 0.9076\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.3638\t Accuracy 0.9038\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.3747\t Accuracy 0.8997\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.3722\t Accuracy 0.9002\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.3696\t Accuracy 0.9010\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.3686\t Accuracy 0.9011\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.3710\t Accuracy 0.8995\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.3695\t Accuracy 0.8999\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.3690\t Accuracy 0.9000\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.3692\t Accuracy 0.8994\n",
      "\n",
      "Epoch [2]\t Average training loss 0.3685\t Average training accuracy 0.8992\n",
      "Epoch [2]\t Average validation loss 0.2814\t Average validation accuracy 0.9310\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.3196\t Accuracy 0.9400\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.3222\t Accuracy 0.9143\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.3324\t Accuracy 0.9109\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.3443\t Accuracy 0.9072\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.3419\t Accuracy 0.9074\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.3403\t Accuracy 0.9080\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.3400\t Accuracy 0.9078\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.3427\t Accuracy 0.9062\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.3420\t Accuracy 0.9063\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.3422\t Accuracy 0.9063\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.3430\t Accuracy 0.9058\n",
      "\n",
      "Epoch [3]\t Average training loss 0.3429\t Average training accuracy 0.9055\n",
      "Epoch [3]\t Average validation loss 0.2658\t Average validation accuracy 0.9326\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.2959\t Accuracy 0.9500\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.3035\t Accuracy 0.9192\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.3143\t Accuracy 0.9148\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.3264\t Accuracy 0.9109\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.3240\t Accuracy 0.9113\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.3228\t Accuracy 0.9119\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.3229\t Accuracy 0.9120\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.3255\t Accuracy 0.9104\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.3252\t Accuracy 0.9104\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.3257\t Accuracy 0.9102\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.3267\t Accuracy 0.9098\n",
      "\n",
      "Epoch [4]\t Average training loss 0.3269\t Average training accuracy 0.9096\n",
      "Epoch [4]\t Average validation loss 0.2554\t Average validation accuracy 0.9352\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.2799\t Accuracy 0.9500\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.2909\t Accuracy 0.9235\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.3019\t Accuracy 0.9188\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.3139\t Accuracy 0.9143\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.3115\t Accuracy 0.9147\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.3106\t Accuracy 0.9153\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.3108\t Accuracy 0.9156\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.3134\t Accuracy 0.9141\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.3133\t Accuracy 0.9138\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.3138\t Accuracy 0.9137\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.3150\t Accuracy 0.9133\n",
      "\n",
      "Epoch [5]\t Average training loss 0.3153\t Average training accuracy 0.9130\n",
      "Epoch [5]\t Average validation loss 0.2476\t Average validation accuracy 0.9366\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.2682\t Accuracy 0.9500\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.2814\t Accuracy 0.9253\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.2924\t Accuracy 0.9211\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.3043\t Accuracy 0.9166\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.3019\t Accuracy 0.9171\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.3012\t Accuracy 0.9178\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.3014\t Accuracy 0.9181\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.3039\t Accuracy 0.9168\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.3039\t Accuracy 0.9164\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.3046\t Accuracy 0.9162\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.3059\t Accuracy 0.9158\n",
      "\n",
      "Epoch [6]\t Average training loss 0.3062\t Average training accuracy 0.9156\n",
      "Epoch [6]\t Average validation loss 0.2412\t Average validation accuracy 0.9372\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.2591\t Accuracy 0.9500\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.2738\t Accuracy 0.9286\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.2847\t Accuracy 0.9238\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.2963\t Accuracy 0.9193\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.2940\t Accuracy 0.9199\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.2934\t Accuracy 0.9204\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.2937\t Accuracy 0.9206\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.2961\t Accuracy 0.9193\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.2962\t Accuracy 0.9188\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.2969\t Accuracy 0.9186\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.2982\t Accuracy 0.9180\n",
      "\n",
      "Epoch [7]\t Average training loss 0.2987\t Average training accuracy 0.9178\n",
      "Epoch [7]\t Average validation loss 0.2359\t Average validation accuracy 0.9376\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.2518\t Accuracy 0.9500\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.2673\t Accuracy 0.9308\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.2781\t Accuracy 0.9259\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.2895\t Accuracy 0.9215\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.2872\t Accuracy 0.9220\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.2868\t Accuracy 0.9224\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.2871\t Accuracy 0.9225\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.2894\t Accuracy 0.9212\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.2895\t Accuracy 0.9207\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.2903\t Accuracy 0.9205\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.2916\t Accuracy 0.9201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8]\t Average training loss 0.2921\t Average training accuracy 0.9199\n",
      "Epoch [8]\t Average validation loss 0.2312\t Average validation accuracy 0.9400\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.2457\t Accuracy 0.9500\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.2616\t Accuracy 0.9324\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.2723\t Accuracy 0.9272\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.2834\t Accuracy 0.9230\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.2812\t Accuracy 0.9235\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.2809\t Accuracy 0.9239\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.2813\t Accuracy 0.9241\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.2834\t Accuracy 0.9229\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.2836\t Accuracy 0.9224\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.2844\t Accuracy 0.9222\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.2858\t Accuracy 0.9219\n",
      "\n",
      "Epoch [9]\t Average training loss 0.2863\t Average training accuracy 0.9216\n",
      "Epoch [9]\t Average validation loss 0.2270\t Average validation accuracy 0.9418\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.2405\t Accuracy 0.9500\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.2565\t Accuracy 0.9337\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.2670\t Accuracy 0.9289\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.2780\t Accuracy 0.9246\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.2759\t Accuracy 0.9252\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.2756\t Accuracy 0.9255\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.2760\t Accuracy 0.9257\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.2781\t Accuracy 0.9245\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.2783\t Accuracy 0.9241\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.2791\t Accuracy 0.9239\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.2805\t Accuracy 0.9236\n",
      "\n",
      "Epoch [10]\t Average training loss 0.2811\t Average training accuracy 0.9233\n",
      "Epoch [10]\t Average validation loss 0.2233\t Average validation accuracy 0.9430\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.2360\t Accuracy 0.9500\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.2519\t Accuracy 0.9343\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.2623\t Accuracy 0.9302\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.2730\t Accuracy 0.9260\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.2710\t Accuracy 0.9266\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.2708\t Accuracy 0.9271\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.2713\t Accuracy 0.9273\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.2733\t Accuracy 0.9262\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.2735\t Accuracy 0.9258\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.2743\t Accuracy 0.9256\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.2758\t Accuracy 0.9252\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2763\t Average training accuracy 0.9249\n",
      "Epoch [11]\t Average validation loss 0.2198\t Average validation accuracy 0.9436\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.2320\t Accuracy 0.9500\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.2476\t Accuracy 0.9353\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.2580\t Accuracy 0.9310\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.2685\t Accuracy 0.9270\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.2666\t Accuracy 0.9276\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.2664\t Accuracy 0.9282\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.2669\t Accuracy 0.9285\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.2688\t Accuracy 0.9274\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.2691\t Accuracy 0.9271\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.2699\t Accuracy 0.9269\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.2714\t Accuracy 0.9264\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2719\t Average training accuracy 0.9261\n",
      "Epoch [12]\t Average validation loss 0.2167\t Average validation accuracy 0.9450\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.2285\t Accuracy 0.9500\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.2438\t Accuracy 0.9365\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.2540\t Accuracy 0.9321\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.2643\t Accuracy 0.9283\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.2625\t Accuracy 0.9291\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.2623\t Accuracy 0.9295\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.2629\t Accuracy 0.9298\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.2647\t Accuracy 0.9287\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.2650\t Accuracy 0.9285\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.2659\t Accuracy 0.9282\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.2673\t Accuracy 0.9278\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2679\t Average training accuracy 0.9276\n",
      "Epoch [13]\t Average validation loss 0.2138\t Average validation accuracy 0.9464\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.2253\t Accuracy 0.9500\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.2401\t Accuracy 0.9369\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.2503\t Accuracy 0.9329\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.2604\t Accuracy 0.9291\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.2587\t Accuracy 0.9300\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.2586\t Accuracy 0.9302\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.2591\t Accuracy 0.9304\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.2610\t Accuracy 0.9297\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.2612\t Accuracy 0.9293\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.2621\t Accuracy 0.9291\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.2636\t Accuracy 0.9288\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2641\t Average training accuracy 0.9285\n",
      "Epoch [14]\t Average validation loss 0.2111\t Average validation accuracy 0.9478\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.2225\t Accuracy 0.9500\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.2368\t Accuracy 0.9384\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.2469\t Accuracy 0.9344\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.2568\t Accuracy 0.9307\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.2551\t Accuracy 0.9313\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.2551\t Accuracy 0.9316\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.2557\t Accuracy 0.9316\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.2574\t Accuracy 0.9309\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.2577\t Accuracy 0.9304\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.2586\t Accuracy 0.9302\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.2601\t Accuracy 0.9298\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2606\t Average training accuracy 0.9296\n",
      "Epoch [15]\t Average validation loss 0.2086\t Average validation accuracy 0.9486\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.2199\t Accuracy 0.9500\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.2337\t Accuracy 0.9402\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.2437\t Accuracy 0.9362\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.2534\t Accuracy 0.9325\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.2518\t Accuracy 0.9329\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.2518\t Accuracy 0.9329\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.2524\t Accuracy 0.9329\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.2542\t Accuracy 0.9321\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.2545\t Accuracy 0.9316\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.2554\t Accuracy 0.9313\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.2569\t Accuracy 0.9309\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2574\t Average training accuracy 0.9308\n",
      "Epoch [16]\t Average validation loss 0.2062\t Average validation accuracy 0.9490\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.2175\t Accuracy 0.9600\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.2308\t Accuracy 0.9416\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.2407\t Accuracy 0.9372\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.2503\t Accuracy 0.9336\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.2487\t Accuracy 0.9340\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.2487\t Accuracy 0.9340\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.2494\t Accuracy 0.9340\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.2511\t Accuracy 0.9331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.2514\t Accuracy 0.9327\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.2523\t Accuracy 0.9323\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.2538\t Accuracy 0.9318\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2544\t Average training accuracy 0.9317\n",
      "Epoch [17]\t Average validation loss 0.2041\t Average validation accuracy 0.9490\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.2152\t Accuracy 0.9600\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.2280\t Accuracy 0.9424\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.2379\t Accuracy 0.9381\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.2473\t Accuracy 0.9343\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.2459\t Accuracy 0.9347\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.2459\t Accuracy 0.9348\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.2466\t Accuracy 0.9346\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.2483\t Accuracy 0.9338\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.2485\t Accuracy 0.9335\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.2495\t Accuracy 0.9331\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.2510\t Accuracy 0.9326\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2515\t Average training accuracy 0.9325\n",
      "Epoch [18]\t Average validation loss 0.2020\t Average validation accuracy 0.9496\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.2132\t Accuracy 0.9600\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.2255\t Accuracy 0.9427\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.2353\t Accuracy 0.9393\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.2446\t Accuracy 0.9354\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.2432\t Accuracy 0.9356\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.2432\t Accuracy 0.9358\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.2439\t Accuracy 0.9355\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.2456\t Accuracy 0.9348\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.2459\t Accuracy 0.9344\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.2468\t Accuracy 0.9340\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.2484\t Accuracy 0.9334\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2489\t Average training accuracy 0.9333\n",
      "Epoch [19]\t Average validation loss 0.2001\t Average validation accuracy 0.9502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T16:06:58.149988Z",
     "start_time": "2021-10-17T16:06:11.550371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9359.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 MLP with Softmax Cross-Entropy Loss and ReLU Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using ReLU activation function and Softmax cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T16:06:58.192664Z",
     "start_time": "2021-10-17T16:06:58.154875Z"
    }
   },
   "outputs": [],
   "source": [
    "reluMLP = Network()\n",
    "# Build ReLUMLP with FCLayer and ReLULayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T16:11:00.995685Z",
     "start_time": "2021-10-17T16:06:58.194619Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.4622\t Accuracy 0.1300\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 1.0280\t Accuracy 0.7249\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.7698\t Accuracy 0.7943\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.6610\t Accuracy 0.8206\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.5889\t Accuracy 0.8402\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.5402\t Accuracy 0.8524\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.5055\t Accuracy 0.8617\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.4808\t Accuracy 0.8679\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.4593\t Accuracy 0.8732\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.4415\t Accuracy 0.8778\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.4279\t Accuracy 0.8817\n",
      "\n",
      "Epoch [0]\t Average training loss 0.4153\t Average training accuracy 0.8851\n",
      "Epoch [0]\t Average validation loss 0.2119\t Average validation accuracy 0.9444\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.2629\t Accuracy 0.9500\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.2415\t Accuracy 0.9335\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.2483\t Accuracy 0.9309\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.2534\t Accuracy 0.9283\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.2475\t Accuracy 0.9306\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.2448\t Accuracy 0.9313\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.2430\t Accuracy 0.9318\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.2418\t Accuracy 0.9323\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.2394\t Accuracy 0.9330\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.2376\t Accuracy 0.9334\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.2373\t Accuracy 0.9336\n",
      "\n",
      "Epoch [1]\t Average training loss 0.2356\t Average training accuracy 0.9343\n",
      "Epoch [1]\t Average validation loss 0.1675\t Average validation accuracy 0.9566\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.1994\t Accuracy 0.9600\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.1848\t Accuracy 0.9510\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.1922\t Accuracy 0.9477\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.1962\t Accuracy 0.9459\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.1931\t Accuracy 0.9465\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.1919\t Accuracy 0.9468\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.1918\t Accuracy 0.9466\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.1917\t Accuracy 0.9467\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.1903\t Accuracy 0.9472\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.1897\t Accuracy 0.9472\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.1906\t Accuracy 0.9466\n",
      "\n",
      "Epoch [2]\t Average training loss 0.1896\t Average training accuracy 0.9472\n",
      "Epoch [2]\t Average validation loss 0.1454\t Average validation accuracy 0.9626\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.1666\t Accuracy 0.9600\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.1544\t Accuracy 0.9625\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.1618\t Accuracy 0.9573\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.1646\t Accuracy 0.9561\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.1628\t Accuracy 0.9564\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.1620\t Accuracy 0.9564\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.1627\t Accuracy 0.9561\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.1630\t Accuracy 0.9558\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.1620\t Accuracy 0.9560\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.1620\t Accuracy 0.9559\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.1632\t Accuracy 0.9554\n",
      "\n",
      "Epoch [3]\t Average training loss 0.1626\t Average training accuracy 0.9558\n",
      "Epoch [3]\t Average validation loss 0.1315\t Average validation accuracy 0.9670\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.1446\t Accuracy 0.9700\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.1356\t Accuracy 0.9669\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.1424\t Accuracy 0.9621\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.1444\t Accuracy 0.9613\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.1432\t Accuracy 0.9615\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.1425\t Accuracy 0.9614\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.1436\t Accuracy 0.9610\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.1441\t Accuracy 0.9610\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.1434\t Accuracy 0.9612\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.1436\t Accuracy 0.9610\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.1450\t Accuracy 0.9605\n",
      "\n",
      "Epoch [4]\t Average training loss 0.1445\t Average training accuracy 0.9608\n",
      "Epoch [4]\t Average validation loss 0.1219\t Average validation accuracy 0.9702\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.1298\t Accuracy 0.9700\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.1225\t Accuracy 0.9710\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.1287\t Accuracy 0.9670\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.1299\t Accuracy 0.9661\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.1293\t Accuracy 0.9663\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.1287\t Accuracy 0.9659\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.1299\t Accuracy 0.9655\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.1305\t Accuracy 0.9655\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.1299\t Accuracy 0.9655\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.1303\t Accuracy 0.9653\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.1318\t Accuracy 0.9647\n",
      "\n",
      "Epoch [5]\t Average training loss 0.1314\t Average training accuracy 0.9650\n",
      "Epoch [5]\t Average validation loss 0.1148\t Average validation accuracy 0.9714\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.1188\t Accuracy 0.9700\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.1125\t Accuracy 0.9722\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.1183\t Accuracy 0.9697\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.1190\t Accuracy 0.9689\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.1187\t Accuracy 0.9692\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.1182\t Accuracy 0.9690\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.1195\t Accuracy 0.9685\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.1201\t Accuracy 0.9683\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.1197\t Accuracy 0.9684\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.1202\t Accuracy 0.9681\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.1216\t Accuracy 0.9676\n",
      "\n",
      "Epoch [6]\t Average training loss 0.1214\t Average training accuracy 0.9678\n",
      "Epoch [6]\t Average validation loss 0.1096\t Average validation accuracy 0.9720\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.1111\t Accuracy 0.9800\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.1048\t Accuracy 0.9737\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.1102\t Accuracy 0.9717\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.1105\t Accuracy 0.9714\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.1105\t Accuracy 0.9717\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.1101\t Accuracy 0.9716\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.1114\t Accuracy 0.9711\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.1120\t Accuracy 0.9710\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.1118\t Accuracy 0.9709\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.1123\t Accuracy 0.9707\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.1137\t Accuracy 0.9702\n",
      "\n",
      "Epoch [7]\t Average training loss 0.1135\t Average training accuracy 0.9702\n",
      "Epoch [7]\t Average validation loss 0.1054\t Average validation accuracy 0.9720\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.1043\t Accuracy 0.9800\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.0986\t Accuracy 0.9759\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.1037\t Accuracy 0.9744\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.1038\t Accuracy 0.9742\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.1040\t Accuracy 0.9742\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.1035\t Accuracy 0.9741\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.1049\t Accuracy 0.9737\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.1055\t Accuracy 0.9734\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.1054\t Accuracy 0.9733\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.1060\t Accuracy 0.9730\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.1074\t Accuracy 0.9726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8]\t Average training loss 0.1071\t Average training accuracy 0.9726\n",
      "Epoch [8]\t Average validation loss 0.1019\t Average validation accuracy 0.9734\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0989\t Accuracy 0.9800\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.0937\t Accuracy 0.9773\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.0985\t Accuracy 0.9754\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.0983\t Accuracy 0.9752\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.0987\t Accuracy 0.9752\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.0983\t Accuracy 0.9751\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.0996\t Accuracy 0.9747\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.1003\t Accuracy 0.9746\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.1002\t Accuracy 0.9746\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.1009\t Accuracy 0.9745\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.1022\t Accuracy 0.9741\n",
      "\n",
      "Epoch [9]\t Average training loss 0.1020\t Average training accuracy 0.9741\n",
      "Epoch [9]\t Average validation loss 0.0992\t Average validation accuracy 0.9742\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0943\t Accuracy 0.9800\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0895\t Accuracy 0.9780\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.0942\t Accuracy 0.9762\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.0938\t Accuracy 0.9764\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.0943\t Accuracy 0.9764\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.0939\t Accuracy 0.9765\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.0952\t Accuracy 0.9761\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.0959\t Accuracy 0.9761\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.0959\t Accuracy 0.9760\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.0965\t Accuracy 0.9758\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.0978\t Accuracy 0.9754\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0976\t Average training accuracy 0.9754\n",
      "Epoch [10]\t Average validation loss 0.0968\t Average validation accuracy 0.9746\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0908\t Accuracy 0.9800\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0860\t Accuracy 0.9780\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0906\t Accuracy 0.9768\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.0901\t Accuracy 0.9770\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.0906\t Accuracy 0.9770\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.0903\t Accuracy 0.9773\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.0916\t Accuracy 0.9768\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.0922\t Accuracy 0.9768\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.0922\t Accuracy 0.9767\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.0929\t Accuracy 0.9766\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.0941\t Accuracy 0.9762\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0940\t Average training accuracy 0.9763\n",
      "Epoch [11]\t Average validation loss 0.0948\t Average validation accuracy 0.9752\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0879\t Accuracy 0.9900\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0830\t Accuracy 0.9788\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0875\t Accuracy 0.9777\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0869\t Accuracy 0.9779\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0875\t Accuracy 0.9782\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0871\t Accuracy 0.9784\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0884\t Accuracy 0.9780\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0890\t Accuracy 0.9781\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0891\t Accuracy 0.9779\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0898\t Accuracy 0.9777\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.0910\t Accuracy 0.9774\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0908\t Average training accuracy 0.9773\n",
      "Epoch [12]\t Average validation loss 0.0931\t Average validation accuracy 0.9756\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0853\t Accuracy 0.9900\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0805\t Accuracy 0.9792\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0849\t Accuracy 0.9784\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0841\t Accuracy 0.9785\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0847\t Accuracy 0.9789\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0844\t Accuracy 0.9791\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0857\t Accuracy 0.9788\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0862\t Accuracy 0.9788\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0864\t Accuracy 0.9787\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0871\t Accuracy 0.9785\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0882\t Accuracy 0.9782\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0881\t Average training accuracy 0.9781\n",
      "Epoch [13]\t Average validation loss 0.0917\t Average validation accuracy 0.9758\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0829\t Accuracy 0.9900\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0784\t Accuracy 0.9804\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0826\t Accuracy 0.9797\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0817\t Accuracy 0.9797\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0824\t Accuracy 0.9798\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0821\t Accuracy 0.9799\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0833\t Accuracy 0.9797\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0838\t Accuracy 0.9797\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0840\t Accuracy 0.9796\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0847\t Accuracy 0.9794\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0858\t Accuracy 0.9790\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0857\t Average training accuracy 0.9790\n",
      "Epoch [14]\t Average validation loss 0.0903\t Average validation accuracy 0.9760\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0810\t Accuracy 0.9900\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0766\t Accuracy 0.9808\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0807\t Accuracy 0.9805\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0797\t Accuracy 0.9805\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0804\t Accuracy 0.9805\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0801\t Accuracy 0.9806\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0812\t Accuracy 0.9805\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0818\t Accuracy 0.9806\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0820\t Accuracy 0.9804\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0826\t Accuracy 0.9802\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0838\t Accuracy 0.9799\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0836\t Average training accuracy 0.9798\n",
      "Epoch [15]\t Average validation loss 0.0893\t Average validation accuracy 0.9764\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0795\t Accuracy 0.9900\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0749\t Accuracy 0.9820\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0790\t Accuracy 0.9815\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0779\t Accuracy 0.9813\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0786\t Accuracy 0.9811\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0783\t Accuracy 0.9812\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0794\t Accuracy 0.9812\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0799\t Accuracy 0.9812\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0802\t Accuracy 0.9811\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0808\t Accuracy 0.9809\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0819\t Accuracy 0.9805\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0818\t Average training accuracy 0.9804\n",
      "Epoch [16]\t Average validation loss 0.0884\t Average validation accuracy 0.9766\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0780\t Accuracy 0.9900\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0735\t Accuracy 0.9820\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0774\t Accuracy 0.9819\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0764\t Accuracy 0.9817\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0771\t Accuracy 0.9817\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0767\t Accuracy 0.9818\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0779\t Accuracy 0.9817\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0783\t Accuracy 0.9817\n",
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0786\t Accuracy 0.9816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0792\t Accuracy 0.9814\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0803\t Accuracy 0.9810\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0802\t Average training accuracy 0.9809\n",
      "Epoch [17]\t Average validation loss 0.0875\t Average validation accuracy 0.9772\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0767\t Accuracy 0.9900\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0722\t Accuracy 0.9824\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0761\t Accuracy 0.9823\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0750\t Accuracy 0.9822\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0757\t Accuracy 0.9820\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0754\t Accuracy 0.9821\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0765\t Accuracy 0.9820\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0769\t Accuracy 0.9820\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0772\t Accuracy 0.9819\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0778\t Accuracy 0.9816\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0789\t Accuracy 0.9813\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0788\t Average training accuracy 0.9812\n",
      "Epoch [18]\t Average validation loss 0.0868\t Average validation accuracy 0.9776\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0752\t Accuracy 0.9900\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0710\t Accuracy 0.9827\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0749\t Accuracy 0.9825\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0738\t Accuracy 0.9824\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0744\t Accuracy 0.9823\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0741\t Accuracy 0.9823\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0752\t Accuracy 0.9823\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0757\t Accuracy 0.9823\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0760\t Accuracy 0.9822\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0766\t Accuracy 0.9819\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0776\t Accuracy 0.9816\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0775\t Average training accuracy 0.9814\n",
      "Epoch [19]\t Average validation loss 0.0862\t Average validation accuracy 0.9770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T16:12:04.936543Z",
     "start_time": "2021-10-17T16:11:01.002505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9748.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T16:12:05.480400Z",
     "start_time": "2021-10-17T16:12:04.943165Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\n",
    "                   'relu': [relu_loss, relu_acc]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~~You have finished homework3, congratulations!~~  \n",
    "\n",
    "**Next, according to the requirements 4) of report:**\n",
    "### **You need to construct a two-hidden-layer MLP, using any activation function and loss function.**\n",
    "\n",
    "**Note: Please insert some new cells blow (using '+' bottom in the toolbar) refer to above codes. Do not modify the former code directly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T16:12:05.488660Z",
     "start_time": "2021-10-17T16:12:05.483547Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "max_epoch = 20\n",
    "init_std = 0.01\n",
    "\n",
    "learning_rate_SGD = 0.001\n",
    "weight_decay = 0.1\n",
    "\n",
    "disp_freq = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T16:12:05.545430Z",
     "start_time": "2021-10-17T16:12:05.491141Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = SoftmaxCrossEntropyLossLayer()\n",
    "\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)\n",
    "\n",
    "sigmoidMLP2 = Network()\n",
    "# Build MLP with FCLayer and SigmoidLayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "sigmoidMLP2.add(FCLayer(784, 256))\n",
    "sigmoidMLP2.add(SigmoidLayer())\n",
    "sigmoidMLP2.add(FCLayer(256, 64))\n",
    "sigmoidMLP2.add(SigmoidLayer())\n",
    "sigmoidMLP2.add(FCLayer(64, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T16:17:14.242680Z",
     "start_time": "2021-10-17T16:12:05.547735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.7336\t Accuracy 0.1100\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 2.2146\t Accuracy 0.3265\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 2.0911\t Accuracy 0.4555\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 1.9665\t Accuracy 0.5290\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 1.8386\t Accuracy 0.5790\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 1.7134\t Accuracy 0.6139\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 1.6016\t Accuracy 0.6426\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 1.5101\t Accuracy 0.6639\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 1.4258\t Accuracy 0.6827\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 1.3535\t Accuracy 0.6985\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 1.2902\t Accuracy 0.7116\n",
      "\n",
      "Epoch [0]\t Average training loss 1.2345\t Average training accuracy 0.7233\n",
      "Epoch [0]\t Average validation loss 0.5671\t Average validation accuracy 0.8838\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.6370\t Accuracy 0.8500\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.6110\t Accuracy 0.8547\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.6035\t Accuracy 0.8568\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.6005\t Accuracy 0.8549\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.5892\t Accuracy 0.8558\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.5756\t Accuracy 0.8592\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.5656\t Accuracy 0.8607\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.5606\t Accuracy 0.8613\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.5512\t Accuracy 0.8632\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.5436\t Accuracy 0.8647\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.5371\t Accuracy 0.8656\n",
      "\n",
      "Epoch [1]\t Average training loss 0.5304\t Average training accuracy 0.8670\n",
      "Epoch [1]\t Average validation loss 0.3737\t Average validation accuracy 0.9120\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.4392\t Accuracy 0.9000\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.4222\t Accuracy 0.8922\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.4289\t Accuracy 0.8900\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.4374\t Accuracy 0.8856\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.4335\t Accuracy 0.8863\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.4282\t Accuracy 0.8885\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.4259\t Accuracy 0.8891\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.4268\t Accuracy 0.8882\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.4237\t Accuracy 0.8888\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.4217\t Accuracy 0.8890\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.4206\t Accuracy 0.8888\n",
      "\n",
      "Epoch [2]\t Average training loss 0.4187\t Average training accuracy 0.8890\n",
      "Epoch [2]\t Average validation loss 0.3176\t Average validation accuracy 0.9206\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.3680\t Accuracy 0.9300\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.3609\t Accuracy 0.9082\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.3705\t Accuracy 0.9025\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.3820\t Accuracy 0.8985\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.3791\t Accuracy 0.8987\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.3761\t Accuracy 0.8997\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.3755\t Accuracy 0.8996\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.3776\t Accuracy 0.8982\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.3762\t Accuracy 0.8982\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.3756\t Accuracy 0.8982\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.3759\t Accuracy 0.8978\n",
      "\n",
      "Epoch [3]\t Average training loss 0.3754\t Average training accuracy 0.8976\n",
      "Epoch [3]\t Average validation loss 0.2919\t Average validation accuracy 0.9264\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.3315\t Accuracy 0.9400\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.3314\t Accuracy 0.9165\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.3421\t Accuracy 0.9104\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.3546\t Accuracy 0.9059\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.3519\t Accuracy 0.9064\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.3498\t Accuracy 0.9071\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.3498\t Accuracy 0.9069\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.3522\t Accuracy 0.9052\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.3514\t Accuracy 0.9050\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.3515\t Accuracy 0.9048\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.3523\t Accuracy 0.9043\n",
      "\n",
      "Epoch [4]\t Average training loss 0.3523\t Average training accuracy 0.9040\n",
      "Epoch [4]\t Average validation loss 0.2770\t Average validation accuracy 0.9294\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.3091\t Accuracy 0.9400\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.3139\t Accuracy 0.9184\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.3250\t Accuracy 0.9138\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.3379\t Accuracy 0.9091\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.3353\t Accuracy 0.9097\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.3336\t Accuracy 0.9103\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.3339\t Accuracy 0.9105\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.3364\t Accuracy 0.9088\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.3359\t Accuracy 0.9086\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.3363\t Accuracy 0.9083\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.3375\t Accuracy 0.9078\n",
      "\n",
      "Epoch [5]\t Average training loss 0.3377\t Average training accuracy 0.9075\n",
      "Epoch [5]\t Average validation loss 0.2671\t Average validation accuracy 0.9310\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.2939\t Accuracy 0.9500\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.3021\t Accuracy 0.9206\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.3133\t Accuracy 0.9167\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.3264\t Accuracy 0.9118\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.3238\t Accuracy 0.9122\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.3225\t Accuracy 0.9126\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.3228\t Accuracy 0.9129\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.3254\t Accuracy 0.9115\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.3251\t Accuracy 0.9113\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.3257\t Accuracy 0.9112\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.3270\t Accuracy 0.9107\n",
      "\n",
      "Epoch [6]\t Average training loss 0.3273\t Average training accuracy 0.9103\n",
      "Epoch [6]\t Average validation loss 0.2599\t Average validation accuracy 0.9316\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.2831\t Accuracy 0.9500\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.2934\t Accuracy 0.9225\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.3047\t Accuracy 0.9183\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.3178\t Accuracy 0.9140\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.3152\t Accuracy 0.9144\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.3141\t Accuracy 0.9147\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.3145\t Accuracy 0.9150\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.3170\t Accuracy 0.9134\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.3169\t Accuracy 0.9135\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.3176\t Accuracy 0.9132\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.3190\t Accuracy 0.9128\n",
      "\n",
      "Epoch [7]\t Average training loss 0.3194\t Average training accuracy 0.9125\n",
      "Epoch [7]\t Average validation loss 0.2544\t Average validation accuracy 0.9334\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.2751\t Accuracy 0.9500\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.2868\t Accuracy 0.9249\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.2980\t Accuracy 0.9203\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.3110\t Accuracy 0.9158\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.3084\t Accuracy 0.9162\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.3075\t Accuracy 0.9165\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.3079\t Accuracy 0.9169\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.3104\t Accuracy 0.9154\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.3104\t Accuracy 0.9154\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.3111\t Accuracy 0.9151\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.3126\t Accuracy 0.9147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8]\t Average training loss 0.3131\t Average training accuracy 0.9143\n",
      "Epoch [8]\t Average validation loss 0.2499\t Average validation accuracy 0.9348\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.2689\t Accuracy 0.9500\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.2813\t Accuracy 0.9263\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.2925\t Accuracy 0.9216\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.3054\t Accuracy 0.9170\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.3028\t Accuracy 0.9175\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.3020\t Accuracy 0.9178\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.3024\t Accuracy 0.9181\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.3049\t Accuracy 0.9168\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.3049\t Accuracy 0.9167\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.3057\t Accuracy 0.9164\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.3073\t Accuracy 0.9160\n",
      "\n",
      "Epoch [9]\t Average training loss 0.3078\t Average training accuracy 0.9157\n",
      "Epoch [9]\t Average validation loss 0.2461\t Average validation accuracy 0.9356\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.2639\t Accuracy 0.9500\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.2767\t Accuracy 0.9276\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.2878\t Accuracy 0.9223\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.3006\t Accuracy 0.9181\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.2981\t Accuracy 0.9184\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.2973\t Accuracy 0.9187\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.2978\t Accuracy 0.9190\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.3002\t Accuracy 0.9177\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.3003\t Accuracy 0.9177\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.3011\t Accuracy 0.9173\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.3027\t Accuracy 0.9168\n",
      "\n",
      "Epoch [10]\t Average training loss 0.3032\t Average training accuracy 0.9165\n",
      "Epoch [10]\t Average validation loss 0.2427\t Average validation accuracy 0.9364\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.2598\t Accuracy 0.9500\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.2727\t Accuracy 0.9294\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.2837\t Accuracy 0.9237\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.2964\t Accuracy 0.9192\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.2939\t Accuracy 0.9197\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.2932\t Accuracy 0.9198\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.2936\t Accuracy 0.9202\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.2961\t Accuracy 0.9189\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.2962\t Accuracy 0.9189\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.2970\t Accuracy 0.9186\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.2987\t Accuracy 0.9181\n",
      "\n",
      "Epoch [11]\t Average training loss 0.2992\t Average training accuracy 0.9177\n",
      "Epoch [11]\t Average validation loss 0.2397\t Average validation accuracy 0.9376\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.2563\t Accuracy 0.9500\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.2692\t Accuracy 0.9310\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.2801\t Accuracy 0.9253\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.2926\t Accuracy 0.9205\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.2901\t Accuracy 0.9210\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.2895\t Accuracy 0.9210\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.2900\t Accuracy 0.9213\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.2923\t Accuracy 0.9200\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.2925\t Accuracy 0.9199\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.2934\t Accuracy 0.9196\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.2950\t Accuracy 0.9191\n",
      "\n",
      "Epoch [12]\t Average training loss 0.2955\t Average training accuracy 0.9188\n",
      "Epoch [12]\t Average validation loss 0.2370\t Average validation accuracy 0.9384\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.2532\t Accuracy 0.9500\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.2659\t Accuracy 0.9320\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.2768\t Accuracy 0.9271\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.2891\t Accuracy 0.9223\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.2867\t Accuracy 0.9227\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.2861\t Accuracy 0.9225\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.2866\t Accuracy 0.9227\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.2889\t Accuracy 0.9213\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.2891\t Accuracy 0.9212\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.2900\t Accuracy 0.9208\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.2916\t Accuracy 0.9203\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2921\t Average training accuracy 0.9200\n",
      "Epoch [13]\t Average validation loss 0.2345\t Average validation accuracy 0.9398\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.2505\t Accuracy 0.9500\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.2629\t Accuracy 0.9325\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.2737\t Accuracy 0.9279\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.2859\t Accuracy 0.9230\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.2835\t Accuracy 0.9238\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.2830\t Accuracy 0.9235\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.2834\t Accuracy 0.9235\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.2857\t Accuracy 0.9223\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.2859\t Accuracy 0.9222\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.2868\t Accuracy 0.9218\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.2885\t Accuracy 0.9212\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2890\t Average training accuracy 0.9210\n",
      "Epoch [14]\t Average validation loss 0.2321\t Average validation accuracy 0.9408\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.2480\t Accuracy 0.9500\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.2601\t Accuracy 0.9331\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.2707\t Accuracy 0.9287\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.2829\t Accuracy 0.9238\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.2805\t Accuracy 0.9244\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.2800\t Accuracy 0.9242\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.2805\t Accuracy 0.9243\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.2827\t Accuracy 0.9231\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.2829\t Accuracy 0.9231\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.2839\t Accuracy 0.9227\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.2855\t Accuracy 0.9221\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2860\t Average training accuracy 0.9220\n",
      "Epoch [15]\t Average validation loss 0.2299\t Average validation accuracy 0.9416\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.2457\t Accuracy 0.9500\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.2574\t Accuracy 0.9343\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.2680\t Accuracy 0.9296\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.2800\t Accuracy 0.9247\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.2777\t Accuracy 0.9253\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.2772\t Accuracy 0.9251\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.2777\t Accuracy 0.9250\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.2799\t Accuracy 0.9239\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.2801\t Accuracy 0.9240\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.2810\t Accuracy 0.9235\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.2827\t Accuracy 0.9230\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2832\t Average training accuracy 0.9228\n",
      "Epoch [16]\t Average validation loss 0.2277\t Average validation accuracy 0.9420\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.2435\t Accuracy 0.9500\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.2549\t Accuracy 0.9343\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.2654\t Accuracy 0.9300\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.2772\t Accuracy 0.9252\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.2750\t Accuracy 0.9262\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.2745\t Accuracy 0.9260\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.2750\t Accuracy 0.9260\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.2772\t Accuracy 0.9249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.2774\t Accuracy 0.9249\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.2784\t Accuracy 0.9246\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.2800\t Accuracy 0.9240\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2805\t Average training accuracy 0.9238\n",
      "Epoch [17]\t Average validation loss 0.2256\t Average validation accuracy 0.9428\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.2415\t Accuracy 0.9500\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.2525\t Accuracy 0.9349\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.2628\t Accuracy 0.9309\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.2746\t Accuracy 0.9258\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.2724\t Accuracy 0.9269\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.2720\t Accuracy 0.9267\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.2725\t Accuracy 0.9268\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.2746\t Accuracy 0.9258\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.2748\t Accuracy 0.9258\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.2758\t Accuracy 0.9255\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.2774\t Accuracy 0.9249\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2779\t Average training accuracy 0.9247\n",
      "Epoch [18]\t Average validation loss 0.2236\t Average validation accuracy 0.9430\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.2395\t Accuracy 0.9500\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.2501\t Accuracy 0.9357\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.2604\t Accuracy 0.9313\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.2720\t Accuracy 0.9264\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.2699\t Accuracy 0.9277\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.2695\t Accuracy 0.9275\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.2700\t Accuracy 0.9276\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.2721\t Accuracy 0.9268\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.2723\t Accuracy 0.9267\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.2733\t Accuracy 0.9264\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.2749\t Accuracy 0.9258\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2755\t Average training accuracy 0.9257\n",
      "Epoch [19]\t Average validation loss 0.2217\t Average validation accuracy 0.9436\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sigmoidMLP2, sig_loss2, sig_acc2 = train(sigmoidMLP2, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T16:18:26.041736Z",
     "start_time": "2021-10-17T16:17:14.248547Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9279.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(sigmoidMLP2, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T16:18:26.124866Z",
     "start_time": "2021-10-17T16:18:26.047530Z"
    }
   },
   "outputs": [],
   "source": [
    "reluMLP2 = Network()\n",
    "# Build MLP with FCLayer and SigmoidLayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "reluMLP2.add(FCLayer(784, 256))\n",
    "reluMLP2.add(ReLULayer())\n",
    "reluMLP2.add(FCLayer(256, 64))\n",
    "reluMLP2.add(ReLULayer())\n",
    "reluMLP2.add(FCLayer(64, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T16:24:02.259863Z",
     "start_time": "2021-10-17T16:18:26.129072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][20]\t Batch [0][550]\t Training Loss 2.7435\t Accuracy 0.1500\n",
      "Epoch [0][20]\t Batch [50][550]\t Training Loss 0.9305\t Accuracy 0.7320\n",
      "Epoch [0][20]\t Batch [100][550]\t Training Loss 0.6796\t Accuracy 0.8057\n",
      "Epoch [0][20]\t Batch [150][550]\t Training Loss 0.5801\t Accuracy 0.8325\n",
      "Epoch [0][20]\t Batch [200][550]\t Training Loss 0.5141\t Accuracy 0.8517\n",
      "Epoch [0][20]\t Batch [250][550]\t Training Loss 0.4676\t Accuracy 0.8647\n",
      "Epoch [0][20]\t Batch [300][550]\t Training Loss 0.4339\t Accuracy 0.8744\n",
      "Epoch [0][20]\t Batch [350][550]\t Training Loss 0.4088\t Accuracy 0.8815\n",
      "Epoch [0][20]\t Batch [400][550]\t Training Loss 0.3868\t Accuracy 0.8880\n",
      "Epoch [0][20]\t Batch [450][550]\t Training Loss 0.3685\t Accuracy 0.8933\n",
      "Epoch [0][20]\t Batch [500][550]\t Training Loss 0.3551\t Accuracy 0.8971\n",
      "\n",
      "Epoch [0]\t Average training loss 0.3421\t Average training accuracy 0.9010\n",
      "Epoch [0]\t Average validation loss 0.1688\t Average validation accuracy 0.9528\n",
      "\n",
      "Epoch [1][20]\t Batch [0][550]\t Training Loss 0.2277\t Accuracy 0.9500\n",
      "Epoch [1][20]\t Batch [50][550]\t Training Loss 0.1670\t Accuracy 0.9565\n",
      "Epoch [1][20]\t Batch [100][550]\t Training Loss 0.1752\t Accuracy 0.9507\n",
      "Epoch [1][20]\t Batch [150][550]\t Training Loss 0.1813\t Accuracy 0.9480\n",
      "Epoch [1][20]\t Batch [200][550]\t Training Loss 0.1770\t Accuracy 0.9495\n",
      "Epoch [1][20]\t Batch [250][550]\t Training Loss 0.1740\t Accuracy 0.9503\n",
      "Epoch [1][20]\t Batch [300][550]\t Training Loss 0.1719\t Accuracy 0.9508\n",
      "Epoch [1][20]\t Batch [350][550]\t Training Loss 0.1699\t Accuracy 0.9515\n",
      "Epoch [1][20]\t Batch [400][550]\t Training Loss 0.1676\t Accuracy 0.9525\n",
      "Epoch [1][20]\t Batch [450][550]\t Training Loss 0.1657\t Accuracy 0.9532\n",
      "Epoch [1][20]\t Batch [500][550]\t Training Loss 0.1656\t Accuracy 0.9530\n",
      "\n",
      "Epoch [1]\t Average training loss 0.1636\t Average training accuracy 0.9538\n",
      "Epoch [1]\t Average validation loss 0.1269\t Average validation accuracy 0.9652\n",
      "\n",
      "Epoch [2][20]\t Batch [0][550]\t Training Loss 0.1468\t Accuracy 0.9500\n",
      "Epoch [2][20]\t Batch [50][550]\t Training Loss 0.1168\t Accuracy 0.9692\n",
      "Epoch [2][20]\t Batch [100][550]\t Training Loss 0.1247\t Accuracy 0.9650\n",
      "Epoch [2][20]\t Batch [150][550]\t Training Loss 0.1297\t Accuracy 0.9637\n",
      "Epoch [2][20]\t Batch [200][550]\t Training Loss 0.1276\t Accuracy 0.9646\n",
      "Epoch [2][20]\t Batch [250][550]\t Training Loss 0.1257\t Accuracy 0.9651\n",
      "Epoch [2][20]\t Batch [300][550]\t Training Loss 0.1251\t Accuracy 0.9653\n",
      "Epoch [2][20]\t Batch [350][550]\t Training Loss 0.1242\t Accuracy 0.9656\n",
      "Epoch [2][20]\t Batch [400][550]\t Training Loss 0.1235\t Accuracy 0.9660\n",
      "Epoch [2][20]\t Batch [450][550]\t Training Loss 0.1227\t Accuracy 0.9663\n",
      "Epoch [2][20]\t Batch [500][550]\t Training Loss 0.1236\t Accuracy 0.9659\n",
      "\n",
      "Epoch [2]\t Average training loss 0.1225\t Average training accuracy 0.9664\n",
      "Epoch [2]\t Average validation loss 0.1083\t Average validation accuracy 0.9708\n",
      "\n",
      "Epoch [3][20]\t Batch [0][550]\t Training Loss 0.1098\t Accuracy 0.9700\n",
      "Epoch [3][20]\t Batch [50][550]\t Training Loss 0.0931\t Accuracy 0.9751\n",
      "Epoch [3][20]\t Batch [100][550]\t Training Loss 0.0996\t Accuracy 0.9734\n",
      "Epoch [3][20]\t Batch [150][550]\t Training Loss 0.1032\t Accuracy 0.9719\n",
      "Epoch [3][20]\t Batch [200][550]\t Training Loss 0.1020\t Accuracy 0.9725\n",
      "Epoch [3][20]\t Batch [250][550]\t Training Loss 0.1005\t Accuracy 0.9729\n",
      "Epoch [3][20]\t Batch [300][550]\t Training Loss 0.1006\t Accuracy 0.9730\n",
      "Epoch [3][20]\t Batch [350][550]\t Training Loss 0.1000\t Accuracy 0.9731\n",
      "Epoch [3][20]\t Batch [400][550]\t Training Loss 0.0998\t Accuracy 0.9734\n",
      "Epoch [3][20]\t Batch [450][550]\t Training Loss 0.0995\t Accuracy 0.9736\n",
      "Epoch [3][20]\t Batch [500][550]\t Training Loss 0.1007\t Accuracy 0.9732\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0999\t Average training accuracy 0.9734\n",
      "Epoch [3]\t Average validation loss 0.0971\t Average validation accuracy 0.9742\n",
      "\n",
      "Epoch [4][20]\t Batch [0][550]\t Training Loss 0.0894\t Accuracy 0.9900\n",
      "Epoch [4][20]\t Batch [50][550]\t Training Loss 0.0789\t Accuracy 0.9782\n",
      "Epoch [4][20]\t Batch [100][550]\t Training Loss 0.0844\t Accuracy 0.9774\n",
      "Epoch [4][20]\t Batch [150][550]\t Training Loss 0.0870\t Accuracy 0.9761\n",
      "Epoch [4][20]\t Batch [200][550]\t Training Loss 0.0863\t Accuracy 0.9766\n",
      "Epoch [4][20]\t Batch [250][550]\t Training Loss 0.0849\t Accuracy 0.9773\n",
      "Epoch [4][20]\t Batch [300][550]\t Training Loss 0.0852\t Accuracy 0.9776\n",
      "Epoch [4][20]\t Batch [350][550]\t Training Loss 0.0847\t Accuracy 0.9774\n",
      "Epoch [4][20]\t Batch [400][550]\t Training Loss 0.0848\t Accuracy 0.9776\n",
      "Epoch [4][20]\t Batch [450][550]\t Training Loss 0.0847\t Accuracy 0.9778\n",
      "Epoch [4][20]\t Batch [500][550]\t Training Loss 0.0859\t Accuracy 0.9775\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0853\t Average training accuracy 0.9776\n",
      "Epoch [4]\t Average validation loss 0.0902\t Average validation accuracy 0.9764\n",
      "\n",
      "Epoch [5][20]\t Batch [0][550]\t Training Loss 0.0760\t Accuracy 0.9900\n",
      "Epoch [5][20]\t Batch [50][550]\t Training Loss 0.0695\t Accuracy 0.9800\n",
      "Epoch [5][20]\t Batch [100][550]\t Training Loss 0.0741\t Accuracy 0.9801\n",
      "Epoch [5][20]\t Batch [150][550]\t Training Loss 0.0760\t Accuracy 0.9791\n",
      "Epoch [5][20]\t Batch [200][550]\t Training Loss 0.0754\t Accuracy 0.9797\n",
      "Epoch [5][20]\t Batch [250][550]\t Training Loss 0.0741\t Accuracy 0.9805\n",
      "Epoch [5][20]\t Batch [300][550]\t Training Loss 0.0745\t Accuracy 0.9808\n",
      "Epoch [5][20]\t Batch [350][550]\t Training Loss 0.0741\t Accuracy 0.9809\n",
      "Epoch [5][20]\t Batch [400][550]\t Training Loss 0.0744\t Accuracy 0.9810\n",
      "Epoch [5][20]\t Batch [450][550]\t Training Loss 0.0743\t Accuracy 0.9810\n",
      "Epoch [5][20]\t Batch [500][550]\t Training Loss 0.0754\t Accuracy 0.9807\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0750\t Average training accuracy 0.9807\n",
      "Epoch [5]\t Average validation loss 0.0856\t Average validation accuracy 0.9774\n",
      "\n",
      "Epoch [6][20]\t Batch [0][550]\t Training Loss 0.0685\t Accuracy 0.9900\n",
      "Epoch [6][20]\t Batch [50][550]\t Training Loss 0.0624\t Accuracy 0.9837\n",
      "Epoch [6][20]\t Batch [100][550]\t Training Loss 0.0666\t Accuracy 0.9835\n",
      "Epoch [6][20]\t Batch [150][550]\t Training Loss 0.0678\t Accuracy 0.9823\n",
      "Epoch [6][20]\t Batch [200][550]\t Training Loss 0.0674\t Accuracy 0.9824\n",
      "Epoch [6][20]\t Batch [250][550]\t Training Loss 0.0662\t Accuracy 0.9831\n",
      "Epoch [6][20]\t Batch [300][550]\t Training Loss 0.0666\t Accuracy 0.9833\n",
      "Epoch [6][20]\t Batch [350][550]\t Training Loss 0.0663\t Accuracy 0.9835\n",
      "Epoch [6][20]\t Batch [400][550]\t Training Loss 0.0666\t Accuracy 0.9834\n",
      "Epoch [6][20]\t Batch [450][550]\t Training Loss 0.0666\t Accuracy 0.9834\n",
      "Epoch [6][20]\t Batch [500][550]\t Training Loss 0.0677\t Accuracy 0.9831\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0673\t Average training accuracy 0.9831\n",
      "Epoch [6]\t Average validation loss 0.0821\t Average validation accuracy 0.9786\n",
      "\n",
      "Epoch [7][20]\t Batch [0][550]\t Training Loss 0.0620\t Accuracy 0.9900\n",
      "Epoch [7][20]\t Batch [50][550]\t Training Loss 0.0570\t Accuracy 0.9863\n",
      "Epoch [7][20]\t Batch [100][550]\t Training Loss 0.0608\t Accuracy 0.9851\n",
      "Epoch [7][20]\t Batch [150][550]\t Training Loss 0.0614\t Accuracy 0.9842\n",
      "Epoch [7][20]\t Batch [200][550]\t Training Loss 0.0611\t Accuracy 0.9843\n",
      "Epoch [7][20]\t Batch [250][550]\t Training Loss 0.0600\t Accuracy 0.9848\n",
      "Epoch [7][20]\t Batch [300][550]\t Training Loss 0.0604\t Accuracy 0.9850\n",
      "Epoch [7][20]\t Batch [350][550]\t Training Loss 0.0601\t Accuracy 0.9851\n",
      "Epoch [7][20]\t Batch [400][550]\t Training Loss 0.0606\t Accuracy 0.9849\n",
      "Epoch [7][20]\t Batch [450][550]\t Training Loss 0.0606\t Accuracy 0.9850\n",
      "Epoch [7][20]\t Batch [500][550]\t Training Loss 0.0616\t Accuracy 0.9848\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0613\t Average training accuracy 0.9847\n",
      "Epoch [7]\t Average validation loss 0.0799\t Average validation accuracy 0.9792\n",
      "\n",
      "Epoch [8][20]\t Batch [0][550]\t Training Loss 0.0572\t Accuracy 0.9900\n",
      "Epoch [8][20]\t Batch [50][550]\t Training Loss 0.0528\t Accuracy 0.9871\n",
      "Epoch [8][20]\t Batch [100][550]\t Training Loss 0.0562\t Accuracy 0.9865\n",
      "Epoch [8][20]\t Batch [150][550]\t Training Loss 0.0566\t Accuracy 0.9859\n",
      "Epoch [8][20]\t Batch [200][550]\t Training Loss 0.0563\t Accuracy 0.9864\n",
      "Epoch [8][20]\t Batch [250][550]\t Training Loss 0.0553\t Accuracy 0.9867\n",
      "Epoch [8][20]\t Batch [300][550]\t Training Loss 0.0557\t Accuracy 0.9867\n",
      "Epoch [8][20]\t Batch [350][550]\t Training Loss 0.0555\t Accuracy 0.9869\n",
      "Epoch [8][20]\t Batch [400][550]\t Training Loss 0.0560\t Accuracy 0.9865\n",
      "Epoch [8][20]\t Batch [450][550]\t Training Loss 0.0560\t Accuracy 0.9865\n",
      "Epoch [8][20]\t Batch [500][550]\t Training Loss 0.0570\t Accuracy 0.9863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8]\t Average training loss 0.0567\t Average training accuracy 0.9862\n",
      "Epoch [8]\t Average validation loss 0.0782\t Average validation accuracy 0.9796\n",
      "\n",
      "Epoch [9][20]\t Batch [0][550]\t Training Loss 0.0533\t Accuracy 0.9900\n",
      "Epoch [9][20]\t Batch [50][550]\t Training Loss 0.0495\t Accuracy 0.9878\n",
      "Epoch [9][20]\t Batch [100][550]\t Training Loss 0.0526\t Accuracy 0.9875\n",
      "Epoch [9][20]\t Batch [150][550]\t Training Loss 0.0529\t Accuracy 0.9873\n",
      "Epoch [9][20]\t Batch [200][550]\t Training Loss 0.0525\t Accuracy 0.9877\n",
      "Epoch [9][20]\t Batch [250][550]\t Training Loss 0.0515\t Accuracy 0.9880\n",
      "Epoch [9][20]\t Batch [300][550]\t Training Loss 0.0519\t Accuracy 0.9881\n",
      "Epoch [9][20]\t Batch [350][550]\t Training Loss 0.0518\t Accuracy 0.9882\n",
      "Epoch [9][20]\t Batch [400][550]\t Training Loss 0.0523\t Accuracy 0.9880\n",
      "Epoch [9][20]\t Batch [450][550]\t Training Loss 0.0524\t Accuracy 0.9879\n",
      "Epoch [9][20]\t Batch [500][550]\t Training Loss 0.0532\t Accuracy 0.9876\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0530\t Average training accuracy 0.9876\n",
      "Epoch [9]\t Average validation loss 0.0768\t Average validation accuracy 0.9796\n",
      "\n",
      "Epoch [10][20]\t Batch [0][550]\t Training Loss 0.0507\t Accuracy 0.9900\n",
      "Epoch [10][20]\t Batch [50][550]\t Training Loss 0.0469\t Accuracy 0.9890\n",
      "Epoch [10][20]\t Batch [100][550]\t Training Loss 0.0497\t Accuracy 0.9888\n",
      "Epoch [10][20]\t Batch [150][550]\t Training Loss 0.0497\t Accuracy 0.9887\n",
      "Epoch [10][20]\t Batch [200][550]\t Training Loss 0.0494\t Accuracy 0.9889\n",
      "Epoch [10][20]\t Batch [250][550]\t Training Loss 0.0484\t Accuracy 0.9892\n",
      "Epoch [10][20]\t Batch [300][550]\t Training Loss 0.0489\t Accuracy 0.9892\n",
      "Epoch [10][20]\t Batch [350][550]\t Training Loss 0.0488\t Accuracy 0.9892\n",
      "Epoch [10][20]\t Batch [400][550]\t Training Loss 0.0493\t Accuracy 0.9891\n",
      "Epoch [10][20]\t Batch [450][550]\t Training Loss 0.0493\t Accuracy 0.9890\n",
      "Epoch [10][20]\t Batch [500][550]\t Training Loss 0.0501\t Accuracy 0.9887\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0499\t Average training accuracy 0.9887\n",
      "Epoch [10]\t Average validation loss 0.0758\t Average validation accuracy 0.9800\n",
      "\n",
      "Epoch [11][20]\t Batch [0][550]\t Training Loss 0.0478\t Accuracy 0.9900\n",
      "Epoch [11][20]\t Batch [50][550]\t Training Loss 0.0446\t Accuracy 0.9894\n",
      "Epoch [11][20]\t Batch [100][550]\t Training Loss 0.0472\t Accuracy 0.9895\n",
      "Epoch [11][20]\t Batch [150][550]\t Training Loss 0.0472\t Accuracy 0.9895\n",
      "Epoch [11][20]\t Batch [200][550]\t Training Loss 0.0468\t Accuracy 0.9898\n",
      "Epoch [11][20]\t Batch [250][550]\t Training Loss 0.0459\t Accuracy 0.9900\n",
      "Epoch [11][20]\t Batch [300][550]\t Training Loss 0.0464\t Accuracy 0.9900\n",
      "Epoch [11][20]\t Batch [350][550]\t Training Loss 0.0463\t Accuracy 0.9899\n",
      "Epoch [11][20]\t Batch [400][550]\t Training Loss 0.0468\t Accuracy 0.9899\n",
      "Epoch [11][20]\t Batch [450][550]\t Training Loss 0.0469\t Accuracy 0.9898\n",
      "Epoch [11][20]\t Batch [500][550]\t Training Loss 0.0476\t Accuracy 0.9895\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0474\t Average training accuracy 0.9895\n",
      "Epoch [11]\t Average validation loss 0.0750\t Average validation accuracy 0.9804\n",
      "\n",
      "Epoch [12][20]\t Batch [0][550]\t Training Loss 0.0457\t Accuracy 0.9900\n",
      "Epoch [12][20]\t Batch [50][550]\t Training Loss 0.0427\t Accuracy 0.9906\n",
      "Epoch [12][20]\t Batch [100][550]\t Training Loss 0.0451\t Accuracy 0.9903\n",
      "Epoch [12][20]\t Batch [150][550]\t Training Loss 0.0449\t Accuracy 0.9903\n",
      "Epoch [12][20]\t Batch [200][550]\t Training Loss 0.0446\t Accuracy 0.9906\n",
      "Epoch [12][20]\t Batch [250][550]\t Training Loss 0.0437\t Accuracy 0.9908\n",
      "Epoch [12][20]\t Batch [300][550]\t Training Loss 0.0442\t Accuracy 0.9908\n",
      "Epoch [12][20]\t Batch [350][550]\t Training Loss 0.0441\t Accuracy 0.9907\n",
      "Epoch [12][20]\t Batch [400][550]\t Training Loss 0.0446\t Accuracy 0.9906\n",
      "Epoch [12][20]\t Batch [450][550]\t Training Loss 0.0447\t Accuracy 0.9905\n",
      "Epoch [12][20]\t Batch [500][550]\t Training Loss 0.0454\t Accuracy 0.9903\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0453\t Average training accuracy 0.9903\n",
      "Epoch [12]\t Average validation loss 0.0742\t Average validation accuracy 0.9804\n",
      "\n",
      "Epoch [13][20]\t Batch [0][550]\t Training Loss 0.0439\t Accuracy 0.9900\n",
      "Epoch [13][20]\t Batch [50][550]\t Training Loss 0.0410\t Accuracy 0.9914\n",
      "Epoch [13][20]\t Batch [100][550]\t Training Loss 0.0434\t Accuracy 0.9909\n",
      "Epoch [13][20]\t Batch [150][550]\t Training Loss 0.0431\t Accuracy 0.9911\n",
      "Epoch [13][20]\t Batch [200][550]\t Training Loss 0.0427\t Accuracy 0.9913\n",
      "Epoch [13][20]\t Batch [250][550]\t Training Loss 0.0418\t Accuracy 0.9915\n",
      "Epoch [13][20]\t Batch [300][550]\t Training Loss 0.0423\t Accuracy 0.9915\n",
      "Epoch [13][20]\t Batch [350][550]\t Training Loss 0.0423\t Accuracy 0.9914\n",
      "Epoch [13][20]\t Batch [400][550]\t Training Loss 0.0428\t Accuracy 0.9913\n",
      "Epoch [13][20]\t Batch [450][550]\t Training Loss 0.0429\t Accuracy 0.9911\n",
      "Epoch [13][20]\t Batch [500][550]\t Training Loss 0.0436\t Accuracy 0.9909\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0434\t Average training accuracy 0.9910\n",
      "Epoch [13]\t Average validation loss 0.0732\t Average validation accuracy 0.9804\n",
      "\n",
      "Epoch [14][20]\t Batch [0][550]\t Training Loss 0.0414\t Accuracy 0.9900\n",
      "Epoch [14][20]\t Batch [50][550]\t Training Loss 0.0396\t Accuracy 0.9914\n",
      "Epoch [14][20]\t Batch [100][550]\t Training Loss 0.0418\t Accuracy 0.9911\n",
      "Epoch [14][20]\t Batch [150][550]\t Training Loss 0.0415\t Accuracy 0.9914\n",
      "Epoch [14][20]\t Batch [200][550]\t Training Loss 0.0411\t Accuracy 0.9917\n",
      "Epoch [14][20]\t Batch [250][550]\t Training Loss 0.0402\t Accuracy 0.9918\n",
      "Epoch [14][20]\t Batch [300][550]\t Training Loss 0.0408\t Accuracy 0.9919\n",
      "Epoch [14][20]\t Batch [350][550]\t Training Loss 0.0407\t Accuracy 0.9918\n",
      "Epoch [14][20]\t Batch [400][550]\t Training Loss 0.0412\t Accuracy 0.9918\n",
      "Epoch [14][20]\t Batch [450][550]\t Training Loss 0.0413\t Accuracy 0.9916\n",
      "Epoch [14][20]\t Batch [500][550]\t Training Loss 0.0420\t Accuracy 0.9915\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0419\t Average training accuracy 0.9915\n",
      "Epoch [14]\t Average validation loss 0.0729\t Average validation accuracy 0.9806\n",
      "\n",
      "Epoch [15][20]\t Batch [0][550]\t Training Loss 0.0402\t Accuracy 0.9900\n",
      "Epoch [15][20]\t Batch [50][550]\t Training Loss 0.0384\t Accuracy 0.9925\n",
      "Epoch [15][20]\t Batch [100][550]\t Training Loss 0.0405\t Accuracy 0.9918\n",
      "Epoch [15][20]\t Batch [150][550]\t Training Loss 0.0400\t Accuracy 0.9921\n",
      "Epoch [15][20]\t Batch [200][550]\t Training Loss 0.0397\t Accuracy 0.9924\n",
      "Epoch [15][20]\t Batch [250][550]\t Training Loss 0.0389\t Accuracy 0.9925\n",
      "Epoch [15][20]\t Batch [300][550]\t Training Loss 0.0394\t Accuracy 0.9924\n",
      "Epoch [15][20]\t Batch [350][550]\t Training Loss 0.0394\t Accuracy 0.9924\n",
      "Epoch [15][20]\t Batch [400][550]\t Training Loss 0.0399\t Accuracy 0.9923\n",
      "Epoch [15][20]\t Batch [450][550]\t Training Loss 0.0400\t Accuracy 0.9921\n",
      "Epoch [15][20]\t Batch [500][550]\t Training Loss 0.0406\t Accuracy 0.9920\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0405\t Average training accuracy 0.9920\n",
      "Epoch [15]\t Average validation loss 0.0723\t Average validation accuracy 0.9810\n",
      "\n",
      "Epoch [16][20]\t Batch [0][550]\t Training Loss 0.0387\t Accuracy 0.9900\n",
      "Epoch [16][20]\t Batch [50][550]\t Training Loss 0.0373\t Accuracy 0.9924\n",
      "Epoch [16][20]\t Batch [100][550]\t Training Loss 0.0393\t Accuracy 0.9919\n",
      "Epoch [16][20]\t Batch [150][550]\t Training Loss 0.0388\t Accuracy 0.9923\n",
      "Epoch [16][20]\t Batch [200][550]\t Training Loss 0.0385\t Accuracy 0.9926\n",
      "Epoch [16][20]\t Batch [250][550]\t Training Loss 0.0377\t Accuracy 0.9927\n",
      "Epoch [16][20]\t Batch [300][550]\t Training Loss 0.0382\t Accuracy 0.9927\n",
      "Epoch [16][20]\t Batch [350][550]\t Training Loss 0.0382\t Accuracy 0.9926\n",
      "Epoch [16][20]\t Batch [400][550]\t Training Loss 0.0387\t Accuracy 0.9926\n",
      "Epoch [16][20]\t Batch [450][550]\t Training Loss 0.0388\t Accuracy 0.9924\n",
      "Epoch [16][20]\t Batch [500][550]\t Training Loss 0.0395\t Accuracy 0.9923\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0393\t Average training accuracy 0.9923\n",
      "Epoch [16]\t Average validation loss 0.0717\t Average validation accuracy 0.9806\n",
      "\n",
      "Epoch [17][20]\t Batch [0][550]\t Training Loss 0.0377\t Accuracy 0.9900\n",
      "Epoch [17][20]\t Batch [50][550]\t Training Loss 0.0364\t Accuracy 0.9927\n",
      "Epoch [17][20]\t Batch [100][550]\t Training Loss 0.0383\t Accuracy 0.9924\n",
      "Epoch [17][20]\t Batch [150][550]\t Training Loss 0.0378\t Accuracy 0.9928\n",
      "Epoch [17][20]\t Batch [200][550]\t Training Loss 0.0374\t Accuracy 0.9931\n",
      "Epoch [17][20]\t Batch [250][550]\t Training Loss 0.0367\t Accuracy 0.9932\n",
      "Epoch [17][20]\t Batch [300][550]\t Training Loss 0.0372\t Accuracy 0.9932\n",
      "Epoch [17][20]\t Batch [350][550]\t Training Loss 0.0371\t Accuracy 0.9931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17][20]\t Batch [400][550]\t Training Loss 0.0377\t Accuracy 0.9931\n",
      "Epoch [17][20]\t Batch [450][550]\t Training Loss 0.0377\t Accuracy 0.9929\n",
      "Epoch [17][20]\t Batch [500][550]\t Training Loss 0.0384\t Accuracy 0.9928\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0383\t Average training accuracy 0.9927\n",
      "Epoch [17]\t Average validation loss 0.0712\t Average validation accuracy 0.9806\n",
      "\n",
      "Epoch [18][20]\t Batch [0][550]\t Training Loss 0.0356\t Accuracy 0.9900\n",
      "Epoch [18][20]\t Batch [50][550]\t Training Loss 0.0355\t Accuracy 0.9927\n",
      "Epoch [18][20]\t Batch [100][550]\t Training Loss 0.0374\t Accuracy 0.9926\n",
      "Epoch [18][20]\t Batch [150][550]\t Training Loss 0.0368\t Accuracy 0.9930\n",
      "Epoch [18][20]\t Batch [200][550]\t Training Loss 0.0364\t Accuracy 0.9932\n",
      "Epoch [18][20]\t Batch [250][550]\t Training Loss 0.0357\t Accuracy 0.9933\n",
      "Epoch [18][20]\t Batch [300][550]\t Training Loss 0.0362\t Accuracy 0.9933\n",
      "Epoch [18][20]\t Batch [350][550]\t Training Loss 0.0362\t Accuracy 0.9934\n",
      "Epoch [18][20]\t Batch [400][550]\t Training Loss 0.0367\t Accuracy 0.9934\n",
      "Epoch [18][20]\t Batch [450][550]\t Training Loss 0.0368\t Accuracy 0.9931\n",
      "Epoch [18][20]\t Batch [500][550]\t Training Loss 0.0375\t Accuracy 0.9930\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0373\t Average training accuracy 0.9930\n",
      "Epoch [18]\t Average validation loss 0.0707\t Average validation accuracy 0.9804\n",
      "\n",
      "Epoch [19][20]\t Batch [0][550]\t Training Loss 0.0337\t Accuracy 0.9900\n",
      "Epoch [19][20]\t Batch [50][550]\t Training Loss 0.0347\t Accuracy 0.9931\n",
      "Epoch [19][20]\t Batch [100][550]\t Training Loss 0.0365\t Accuracy 0.9929\n",
      "Epoch [19][20]\t Batch [150][550]\t Training Loss 0.0359\t Accuracy 0.9934\n",
      "Epoch [19][20]\t Batch [200][550]\t Training Loss 0.0355\t Accuracy 0.9936\n",
      "Epoch [19][20]\t Batch [250][550]\t Training Loss 0.0348\t Accuracy 0.9937\n",
      "Epoch [19][20]\t Batch [300][550]\t Training Loss 0.0354\t Accuracy 0.9936\n",
      "Epoch [19][20]\t Batch [350][550]\t Training Loss 0.0354\t Accuracy 0.9937\n",
      "Epoch [19][20]\t Batch [400][550]\t Training Loss 0.0359\t Accuracy 0.9937\n",
      "Epoch [19][20]\t Batch [450][550]\t Training Loss 0.0360\t Accuracy 0.9935\n",
      "Epoch [19][20]\t Batch [500][550]\t Training Loss 0.0366\t Accuracy 0.9933\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0365\t Average training accuracy 0.9933\n",
      "Epoch [19]\t Average validation loss 0.0704\t Average validation accuracy 0.9804\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reluMLP2, relu_loss2, relu_acc2 = train(reluMLP2, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T16:25:23.288106Z",
     "start_time": "2021-10-17T16:24:02.268910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9793.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(reluMLP2, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T16:25:23.894731Z",
     "start_time": "2021-10-17T16:25:23.299906Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEKCAYAAADaa8itAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAu2ElEQVR4nO3deXwV5dn/8c+VHRL2gCJhs2IBpaJGxC5ItSriArR1odpK61Nsrf6ex6o/tbWKKFVbn9qnrV3QUpdWqd0sVXzUKkJ/FitBUEREEFECWPad7Nfvj5nAISQnJ8mZnJPk+369zuvM3DP3PdeEMFdm7ntmzN0RERFJhoxUByAiIu2HkoqIiCSNkoqIiCSNkoqIiCSNkoqIiCSNkoqIiCRNpEnFzGaZ2SYze6uB5WZmPzGz1Wb2ppmdFLPsCjNbFX6uiCk/2cyWhXV+YmYW5T6IiEjioj5TeRgYF2f5ucCQ8DMV+AWAmfUEbgdOBUYBt5tZj7DOL4Cvx9SL176IiLSiSJOKuy8AtsVZZQLwqAdeBbqbWV/gHOAFd9/m7tuBF4Bx4bKu7v6qB3dtPgpMjHIfREQkcVkp3n4/YF3MfGlYFq+8tJ7yw5jZVIKzH/Lz808eOnRo8qIWEekAFi9evMXdezelTqqTSmTcfSYwE6C4uNhLSkpSHJGISNtiZh80tU6qR3+tB/rHzBeFZfHKi+opFxGRNJDqpDIH+Eo4Cmw0sNPdNwLPAWebWY+wg/5s4Llw2S4zGx2O+voK8NeURS8iIoeI9PKXmT0BjAUKzayUYERXNoC7/xKYC4wHVgP7gK+Gy7aZ2Z3AorCp6e5e2+F/NcGosk7As+FHRETSgHWER9+rT0VEpOnMbLG7FzelTqovf4mISDuipCIiIkmjpCIiIkmjpCIiIkmjpCIiIkmjpCIiIkmjpCIiIkmjpCIiIkmjpCIiIkmjpCIiIknTbh99LyKSsB8Ogb2bDi/P7wM3rmr9eNowJRWRtigZB8GWtpEOMSSrjfrqxyuPIoY0bOPkvhknJ1bpICUV6Vjay4G0pQfBZLSRDjE01oY71FRDdUX4qYTq8pjpsDyeTSsgMwcysyEzN/zOOVhm1jZ+Fi1tI0FKKtJ2pMPBuDX+47sfesA75ICYwEHw9UcPX792uqoisTZ+f3ni+xNFfYBHJwQJoaYavO53zcH5eO7oAbTwSew/Hx1/eUZ2/OX3H9+y7QM8MBoyMsEywu/MOt8ZkNHI4TwZ/yYJUFKR1hF1QqjcH3yqyg79rlsWz9PfrudgXOegHM+PRyS2H/FML4Saypa1Mefa+sszc2P+4s6J38bW91oWQ0vrA1TsCw6UGZlg2fUcTDOC7y3vNtzGmBsP3efYs4ysnIPTj1/ccBsXPXxoMj4sWZfD/7u/4fqDxyS2v0t/1/CywiGNJNeK4DueZPybJEBJRRLT0qTQ2CWKyn2wbxvs3w77a7+3x5Rtj9/+jCMbj6Exb//18INN7EEot0v8+gM/ldh2dnzY8LJPXntwu1m5h8dQ+5k9ueE2rlt+eL2MrOAyTaxp3Rpu4+qFje9HS+s31sZ/vNDyNs74bmJtxHPcpMbXiZdUJv48se3ESyqXPJZYG1H/myQg6jc/jgP+B8gEHnL3e+osHwjMAnoD24DL3b3UzD4LxP4rDQUudfenzOxh4HRgZ7hsirsvjXI/hMYv2VRXQtkuKNsB5bvC6Z3h9M7669a6q0/8s4CsTtC5Z/w2zrwNsjtDVh5kdwo+WZ0gOy/8Dj8/PanhNv5vAn/JxfsPN+mXjdcHeOOJhpd97vbE2oinW1HL2+ho8vs0/EeTNElkScXMMoEHgLOAUmCRmc1x97djVrsPeNTdHzGzM4C7gS+7+zxgZNhOT4LXDT8fU+9Gd/9jVLG3O809y6iuhJ2lsH1t/PZn9A3ONJpr9DehU4/w0zP47tzzYFl2p2C9eAf0z1zf/O23Rck4CLa0jXSIIVlttHTYcLrsR5RtJCjKM5VRwGp3XwNgZrOBCUBsUhkOfDucngc8VU87XwSedfcWHLU6uHhnGfu2wfb3g8Rx4PNB8L2ztPGOUIDir0FeN8jtGnzndT10Oq8b3Duo4fpnTW/yLjVbezmQJuPeiZa2kQ4xJKuNdIghDdtYfIctbmr1KJNKP2BdzHwpcGqddd4APk9wiWwS0MXMern71ph1LgV+VKfeDDO7DXgRuNndy5MaeUfyg8GHzuf3hh6DoOgUGHFRMN1jEDxyfsNtnDMjwgBjY0uDg3G6/McXSVOp7qi/AfiZmU0BFgDrgQN/GptZX2AE8FxMnVuAj4AcYCZwE3DYn7pmNhWYCjBgwIBook9H1VWw+R3YsCT8vB5//XPuhh4Dg8TRfSDkFkQTVzokBBGJXJRJZT3QP2a+KCw7wN03EJypYGYFwBfcfUfMKhcDf3H3ypg6G8PJcjP7DUFiOoy7zyRIOhQXF7dwoHoKxesPuX4lbF19MHlsWAIb34Sq/cE6uV2h7wnx2z/t6sTiaGlSUEIQ6RCiTCqLgCFmNpggmVwKfCl2BTMrBLa5ew3BGcisOm1MDstj6/R1941mZsBE4K1owk8T8fpD7hkAFbuD+ezOcOQnoPircNSJwafnx4Kx/C0cIggoKYhIQiJLKu5eZWbXEFy6ygRmuftyM5sOlLj7HGAscLeZOcHlr2/V1jezQQRnOvPrNP07M+sNGLAU+EZU+5By3sgJ1gmXwFEnBQmk8FjIbOCfU8MlRaSVmDd24GoHiouLvaSkJNVhJMY9uIy1/C+w/CnYGedGuWmN3P8hItICZrbY3YubUifVHfUCQSL56M0wkfwlGM6bkQVHfzZ+UhERSTNKKlGK18l+w7vw77cOJpJta4LnGB09Fj5zAww9L7gBMBn9ISIirURJJUrxOtl/dgpsXRUkksFj4FP/BUPPh/xeh66r/hARaUOUVFKl61Fw2rdg2AWQX9jwehp1JSJtiJJKqlwxJ9URiIgkXUaqA2i3Gnu3gYhIO6SkEoXd/4bHJqY6ChGRVqfLX8n23jz489ehfE/wUqfy3Yevo052EWmnlFSSpboK5t8DC+6D3h+HK/4GfYalOioRkValpJIMuzbAn/4DPngFRl4O438AOfmpjkpEpNUpqbTUqr/DX6ZCZRlM+hWccGmqIxIRSRklleaqroSX7oJXfgx9joOLHobex6Y6KhGRlFJSaY4d6+BPV8K6f8HJX4Vxdx98j7qISAempNJUK5+Fp74ZdMx/4dcw4oupjkhEJG0oqcTT0AMhM7LgW69Br4+1fkwiImlMNz/G09ADIWuqlFBEROoRaVIxs3FmttLMVpvZzfUsH2hmL5rZm2b2spkVxSyrNrOl4WdOTPlgM/tX2ObvzSwnyn0QEZHERZZUzCwTeAA4FxgOTDaz4XVWuw941N0/AUwH7o5Ztt/dR4afC2PK7wXud/djgO3AlVHtg4iINE2UZyqjgNXuvsbdK4DZwIQ66wwHXgqn59Wz/BBmZsAZwB/DokeAickKWEREWibKpNIPWBczXxqWxXoD+Hw4PQnoYma1b6nKM7MSM3vVzCaGZb2AHe5eFadNERFJkVR31N8AnG5mS4DTgfVA7TPjB7p7MfAl4Mdm1qSecTObGialks2bNzcvuoYe/KgHQoqI1CvKIcXrgf4x80Vh2QHuvoHwTMXMCoAvuPuOcNn68HuNmb0MnAj8CehuZlnh2cphbca0PROYCVBcXOzN2gO9dVFEpEmiPFNZBAwJR2vlAJcCh7zu0MwKzaw2hluAWWF5DzPLrV0H+BTwtrs7Qd9L7R2HVwB/jXAfRESkCSI7U3H3KjO7BngOyARmuftyM5sOlLj7HGAscLeZObAA+FZYfRjwKzOrIUh897j72+Gym4DZZnYXsAT4dVT7ICLS0RTf9QJb9lQAkHPkMSc3tX6kd9S7+1xgbp2y22Km/8jBkVyx6/wTGNFAm2sIRpaJiLQbsQfzWIUFOZTcelartVFf/abQY1pEpMNr6cE4yoN5Ywd5d6e6xql2j9vGorXbKKuspqyyhrLKavZXVlMeM19WVc3+ipqEYo1HSUVE2rRUHtATqf/P1VvYU17Fvopq9lZUsbe8ir3l1eyrqGJvRfWB+XhOuvMFqmucmjB5VNccTCSe4DCki365MO7yzAyjU3ZmYo3FoaQiIikTdULYvreCvRXBAX1PeRX7ysPvmAP6vvKqeuvXmvTzV6isrqGyyqmsrqGiuiaYr3YqqoL5eL700L/qLe+UnUl+bhb5uZl0zol/KB4/4kgyzcjIMDLNyMwIprMyjIxwPjPD+OFzKxts47ErR5GXnUleViZ52RnBdPbB6ezMYMzUoJufiRtLY5RURKRZok4IG3fuZ3dZFbv2VwbfZZXsKqtid1nlIeXxnHjnCwnFEU9BbhbZmRlkZxrZmRnkZGWQk5kRlmWQnWX8av6aBuvPnjqagtwsOufUJpEsOmVnkplhh6wX72B+18R6u5gPEy+pfGZI74TaaCklFZEOqqVJIdFLRmWV1WzZU862vRVs3VPB1r0VbA3n4znt7pcaXJadaXTNy6ZLXvxD2O0XDA8O5DlZdM7NJD8nODOonS/IzSIvK5OjvzO3wTYeu/LUuNsA4iaV0Uf3anBZOiosyGlRZ72Sikgb1Jr9CNU1HlwuKg/6BGovIcUz8YFX2Lq3nG17KthbUX9/QU5W/Nvk7v78CLrkZdElL5uutd+dsuial01uVgbBowDj/4X/1U8NjruNdNLQwbywIPEHsSejjdjfH7v3/MUJVwwpqYi0stZMCO7OvopqduyvZMe+Cnbsq2THvkq274v/l+iYH8wLOpArqiirbPqIoC55WQzq1ZleBbn0zM+hsCCHnvm59CrIoTA/l54FOeTnZDL4lobPECaPGtDk7TZXSw/GyT6YN1cy2mgpJRWRJmjtkUYVVTXsq6g6OHoogZFCF/9yITv2V7B9XyU791U22pFcn5MH9jjYB5BzsDM59tLRlx6svwMaErtklCzpcEBPh4N5ulBSkQ4lyn6Equoa9laEQ0XLD71ctLfi4DDSeM760fxDhp5WVjf9sXVmcHRhAd07Z9O9c07w3engdI/w+9Tvv9hgG/dfMrLJ222OdEgIklxKKtKhJHKW4O7s2l/Flr21ncvlYedy/EtGx3z32RbH97HeBYcMMy2IPUOo7XDOyeSSma822MbvrzqtxXEkQglB6qOkIm1GS84yahNFPON+vIBteyvYtreCqpqmnyFc97ljDxz8O+fUjjSKTRDBZaNPTHu+wTZ++eUmP2qp2VqaFJQQpD5KKtIqou6L2LS7jH/vLGfjzv18tKuMj3YGn407yw7M76+M3xdR1KMTJxR1p2dBDr3yc+hVkEOv/NqO5lx65Gfz8Vv/t8H6//m5IQntRzLoLEHSlZKKJCTKvoj3Nu858Eyi8vCZRIc+j6ia8qr4nc2jZhzaP5CVYRzRNY8ju+UxvG9Xzhjah77d8rjrmRUNtvHQFac0uh/JoIQg7ZmSiiSksb4Id2dXWRWbd5ezaXcZm3eXh9PBdzxn/vf8Fsd3x4XHcWS3PPp2y+PIrnn0Ksg97I5lIG5SSYQSgkh8SirSYp++9yU27y6v92wiJyuDPl1y49b/n0tHklvneUSdYp5JlJeVSW52BkO/1/Clpys+OSihWNWPIBItJZUOoCmXrmpqnNLt+1nx0S5WbNzFOxt3885Hu+K3P7AHfbrm0adLLr3DTzCdR9e8LMws7l3PE0b2a96ONYOSgki0lFQ6gHiXrkrWbmPFR7vDBLKLlR/tPvBYDTMY1CufYX27snbrvgbb//GlJ0YSd13JuPQkItGKNKmY2TjgfwheJ/yQu99TZ/lAgvfS9wa2AZe7e6mZjQR+AXQFqoEZ7v77sM7DwOnAzrCZKe6+NMr9aM++GL5joWteFkP7duWi4v4MPbILQ/t25dgjCg48krulj8NWX4RIxxBZUjGzTOAB4CygFFhkZnNi3jUPcB/wqLs/YmZnAHcDXwb2AV9x91VmdhSw2Myec/cdYb0bw1cRSxxV1TUsXLM17jqzphQz9Miu9O2Wd+ABffVRX4SIJCLKM5VRwOrwnfKY2WxgAhCbVIYD3w6n5wFPAbj7u7UruPsGM9tEcDazI8J42wV35/UPtzNn6QaeWbax0UdYnzH0iITaVVIQkUREmVT6Aeti5kuBuk+ZewP4PMElsklAFzPr5e4H/rw2s1FADvBeTL0ZZnYb8CJws7sfNmbVzKYCUwEGDGi9p52mgrvzzke7mfPGBv72xgZKt+8nNyuDM4f14cITjuIbv3091SGKSAeR6o76G4CfmdkUYAGwnqAPBQAz6ws8Blzh7rXjVW8BPiJINDOBm4DpdRt295nhcoqLi5v+zI00EW/k1p+++UnmLN3AnDc2sGrTHjIzjE8fU8h1nzuWs487gi552QfWVQe3iLSGKJPKeqB/zHxRWHaAu28gOFPBzAqAL9T2m5hZV+AZ4Lvu/mpMnY3hZLmZ/YYgMbVb8UZunf7DlwEYNagnd048nvHHH0mvgsPvCdGlKxFpLVEmlUXAEDMbTJBMLgW+FLuCmRUC28KzkFsIRoJhZjnAXwg68f9Yp05fd99oQa/yROCtCPchrd1y7lDOP+Eo+nXvlOpQREQAiP8+zxZw9yrgGuA5YAXwpLsvN7PpZnZhuNpYYKWZvQscAcwIyy8GxgBTzGxp+BkZLvudmS0DlgGFwF1R7UOquce/anfV6R9TQhGRtBJpn4q7zwXm1im7LWb6j8BhQ4Pd/bfAbxto84wkh5l23J2X393Mj55/t/GVRUTSSKo76qWOhe9t5b+fX0nJB9sp6qGzEBFpWyK7/CVN8/qH27n8oX8x+cFXWbd9H3dNPJ6Xrh/b4AgtjdwSkXSkM5UUW75hJz96/l1efGcTvfJzuPW8YVw+eiB52ZmARm6JSNuipJIiqzft5v4XVvHMso10zcvixnM+zpRPDiI/V/8kItJ26QgWoYZuXMzNyqCyuoZO2Zn8nzOO4crPHE23TtkpiFBEJLmUVCLU0I2L5VU1XDXmaK46/WP0zFffiIi0H0oqKXLL+GGpDkFEJOkaHf1lZheYmUaJiYhIoxJJFpcAq8zsB2Y2NOqARESk7Wo0qbj75cCJBI+ef9jMFprZVDPrEnl0IiLSpiR0WcvddxE8TmU20Jfg3Sevm9m1EcbWprk72Zn1v0lRNy6KSHvVaEd9+PDHrwLHAI8Co9x9k5l1JniL40+jDbFt+vuKTVRWO987fzhXfnpwqsMREWkViYz++gJwv7sviC10931mdmU0YbVtZZXV3Pn02wzpU8BXThuY6nBERFpNIkllGlD7YizMrBNwhLuvdfcXowqsLXtwwRo+3LaPx//jVLIzNXBORDqORI54fwBqYuarwzKpx/od+3ng5dWMH3EknzymMNXhiIi0qkSSSpa7H7g1PJxWT3MDvv/MCgC+e97wFEciItL6Ekkqm2Pe1IiZTQC2JNK4mY0zs5VmttrMbq5n+UAze9HM3jSzl82sKGbZFWa2KvxcEVN+spktC9v8Sfha4bTwz9VbeGbZRq4ee4zeyCgiHVIiSeUbwHfM7EMzWwfcBFzVWCUzywQeAM4FhgOTzazun+/3EbyH/hPAdODusG5P4HbgVGAUcLuZ9Qjr/AL4OjAk/IxLYB8iV1ldw7S/Lad/z05MHXN0qsMREUmJRG5+fM/dRxMkhmHu/kl3X51A26OA1e6+JrxkNhuYUGed4cBL4fS8mOXnAC+4+zZ33w68AIwzs75AV3d/1YMXuD8KTEwglsg9tvAD3v33Hr533vAD70IREeloEnqgpJmdBxwH5NVebXL36Y1U6wesi5kvJTjziPUG8HngfwhuqOxiZr0aqNsv/JTWU15fzFOBqQADBgxoJNSW2by7nPtfeJcxx/bmrOFHRLotEZF0lsgDJX9J8PyvawEDLgKSdfPFDcDpZrYEOB1YTzC6rMXcfaa7F7t7ce/evZPRZIN+8L/vUFZVze0XDCeNunhERFpdIn0qn3T3rwDb3f0O4DTg2ATqrQf6x8wXhWUHuPsGd/+8u58IfDcs2xGn7vpwusE2W9uSD7fzh8WlfO3Tg/lY74JUhiIiknKJJJWy8HufmR0FVBI8/6sxi4AhZjbYzHKAS4E5sSuYWWHMY/VvAWaF088BZ5tZj7CD/mzgOXffCOwys9HhqK+vAH9NIJZI1NQ40+Ysp0+XXK49Y0iqwhARSRuJJJW/mVl34IfA68Ba4PHGKrl7FXANQYJYATzp7svNbHrMEOWxwEozexc4ApgR1t0G3EmQmBYB08MygKuBh4DVBE9OfjaBfYjEHxav443SnXxn/DAK9G55EREsGETVwMLgLGK0u/8znM8F8tx9ZyvFlxTFxcVeUlKS1DZ37q/kjPte5uje+Tx51WnqSxGRdsfMFrt7cVPqxD1TcfcagntNaufL21pCicr9L7zL9n0VTLvwOCUUEZFQIpe/XjSzL6TTneuptvKj3Tz26gd86dQBHHdUt1SHIyKSNhJJKlcRPECy3Mx2mdluM9sVcVxpy925fc5bdMnL4vqzPp7qcERE0kqjvcvurtcGx3hm2UZeXbONuyYeT498PVdTRCRWIm9+HFNfed2XdnUE+yqqmPHMCo47qiuTR0V7l76ISFuUyDjYG2Om8wie6bUYOCOSiNLYA/NWs3FnGT+dfCKZGepiEhGpK5HLXxfEzptZf+DHUQWUrtZu2cuDC97n8yf2o3hQz1SHIyKSlppzx14pMCzZgaSj4rteYMueikPK/rxkPQtWbabk1rNSFJWISPpKpE/lp0DtHZIZwEiCO+vbvboJpbFyEZGOLpEzldhb0auAJ9z9lYjiERGRNiyRpPJHoMzdqyF4o6OZdXb3fdGGJiIibU1Cd9QDsS9c7wT8PZpwRESkLUskqeS5+57amXC6c3QhiYhIW5VIUtlrZifVzpjZycD+6EJKH4UF9d8x31C5iEhHl0ifyn8BfzCzDQSvEz6S4PXC7Z6GDYuINE0iNz8uMrOhQO3TE1e6e2W0YYmISFvU6OUvM/sWkO/ub7n7W0CBmV2dSONmNs7MVprZajO7uZ7lA8xsnpktMbM3zWx8WH6ZmS2N+dSY2chw2cthm7XL+jRpj0VEJDKJ9Kl83d131M64+3bg641VMrNMghd8nQsMByab2fA6q91K8JrhEwneYf/zcBu/c/eR7j4S+DLwvrsvjal3We1yd9+UwD6IiEgrSCSpZMa+oCtMFon0VI8CVrv7GnevAGYDE+qs40DXcLobsKGediaHdUVEJM0lklT+F/i9mZ1pZmcCTwDPJlCvH7AuZr40LIs1DbjczEqBucC19bRzSbjNWL8JL319r6E3UprZVDMrMbOSzZs3JxCuiIi0VCJJ5SbgJeAb4WcZh94M2RKTgYfdvQgYDzxmZgdiMrNTgX1hX06ty9x9BPCZ8PPl+hp295nuXuzuxb17905SuCIiEk+jScXda4B/AWsJLmmdAaxIoO31QP+Y+aKwLNaVwJPhdhYSvK+lMGb5pdQ5S3H39eH3buDxMCYREUkDDSYVMzvWzG43s3eAnwIfArj7Z939Zwm0vQgYYmaDzSyHIEHMqbPOh8CZ4faGESSVzeF8BnAxMf0pZpZlZoXhdDZwPvAWIiKSFuLdp/IO8A/gfHdfDWBm1yXasLtXmdk1wHNAJjDL3Zeb2XSgxN3nANcDD4btOjDF3Wsfsz8GWOfua2KazQWeCxNKJsEzyB5MNCYREYmWHTyG11lgNpHg7OJTBJ31s4GH3H1wq0WXJMXFxV5SUtL4iiIicoCZLXb34qbUafDyl7s/5e6XAkOBeQSPa+ljZr8ws7NbFKmIiLRLiXTU73X3x8N31RcBSwhGhImIiBwikSHFB7j79nCo7plRBSQiIm1Xk5KKiIhIPEoqIiKSNEoqIiKSNEoqIiKSNEoqIiKSNEoqIiKSNEoqIiKSNEoqIiKSNEoqIiKSNEoqIiKSNEoqIiKSNEoqIiKSNEoqIiKSNJEmFTMbZ2YrzWy1md1cz/IBZjbPzJaY2ZtmNj4sH2Rm+81safj5ZUydk81sWdjmT8zMotwHERFJXGRJxcwygQeAc4HhwGQzG15ntVuBJ939RIK3TP48Ztl77j4y/HwjpvwXwNeBIeFnXFT7ICIiTRPlmcooYLW7r3H3CoLXEU+os44DXcPpbsCGeA2aWV+gq7u/Gr7L/lFgYlKjFhGRZosyqfQD1sXMl4ZlsaYBl5tZKTAXuDZm2eDwsth8M/tMTJuljbQJgJlNNbMSMyvZvHlzC3ZDREQSleqO+snAw+5eBIwHHjOzDGAjMCC8LPZt4HEz6xqnncOEb6gsdvfi3r17Jz1wERE5XFaEba8H+sfMF4Vlsa4k7BNx94VmlgcUuvsmoDwsX2xm7wHHhvWLGmlTRERSJMozlUXAEDMbbGY5BB3xc+qs8yFwJoCZDQPygM1m1jvs6MfMjibokF/j7huBXWY2Ohz19RXgrxHug4iINEFkZyruXmVm1wDPAZnALHdfbmbTgRJ3nwNcDzxoZtcRdNpPcXc3szHAdDOrBGqAb7j7trDpq4GHgU7As+FHRETSgAWDqNq34uJiLykpSXUYIiJtipktdvfiptRJdUe9iIi0I0oqIiKSNEoqIiKSNEoqIiKSNEoqIiKSNEoqIiKSNEoqIiKSNEoqIiKSNEoqIiKSNEoqIiKSNEoqIiKSNEoqIiKSNEoqIiKSNEoqIiKSNEoqIiKSNEoqIiKSNJEmFTMbZ2YrzWy1md1cz/IBZjbPzJaY2ZtmNj4sP8vMFpvZsvD7jJg6L4dtLg0/faLcBxERSVxkrxMO3zH/AHAWUAosMrM57v52zGq3Ak+6+y/MbDgwFxgEbAEucPcNZnY8wSuJ+8XUu8zd9SpHEZE0E+WZyihgtbuvcfcKYDYwoc46DnQNp7sBGwDcfYm7bwjLlwOdzCw3wlhFRCQJokwq/YB1MfOlHHq2ATANuNzMSgnOUq6tp50vAK+7e3lM2W/CS1/fMzOrb+NmNtXMSsysZPPmzc3eCRERSVyqO+onAw+7exEwHnjMzA7EZGbHAfcCV8XUuczdRwCfCT9frq9hd5/p7sXuXty7d+/IdkBERA6KMqmsB/rHzBeFZbGuBJ4EcPeFQB5QCGBmRcBfgK+4+3u1Fdx9ffi9G3ic4DKbiIikgSiTyiJgiJkNNrMc4FJgTp11PgTOBDCzYQRJZbOZdQeeAW5291dqVzazLDOrTTrZwPnAWxHug4iINEFko7/cvcrMriEYuZUJzHL35WY2HShx9znA9cCDZnYdQaf9FHf3sN4xwG1mdlvY5NnAXuC5MKFkAn8HHmxOfJWVlZSWllJWVtaS3ewQ8vLyKCoqIjs7O9WhiEiaM3dPdQyRKy4u9pKSQ0cgv//++3Tp0oVevXrRQF+/AO7O1q1b2b17N4MHD051OCLSisxssbsXN6VOqjvqU6asrEwJJQFmRq9evXRGJyIJ6bBJBVBCSZB+TiKSqA6dVEREJLki66hvT4rveoEteyoOKy8syKHk1rOa3e6MGTN4/PHHyczMJCMjg1/96lc8+OCDfPvb32b48OEtCTmu8ePH8/jjj9O9e/dDyqdNm0ZBQQE33HBDZNsWkfZNSSUB9SWUeOWJWLhwIU8//TSvv/46ubm5bNmyhYqKCh566KFmt5mouXPnRr4NEemYlFSAO/62nLc37GpW3Ut+tbDe8uFHdeX2C45rsN7GjRspLCwkNzd4pFlhYSEAY8eO5b777qO4uJhf//rX3HvvvXTv3p0TTjiB3NxcfvaznzFlyhQ6derEkiVL2LRpE7NmzeLRRx9l4cKFnHrqqTz88MMAPPHEE3z/+9/H3TnvvPO49957ARg0aBAlJSUUFhYyY8YMHnnkEfr06UP//v05+eSTm/VzEBEB9amkzNlnn826des49thjufrqq5k/f/4hyzds2MCdd97Jq6++yiuvvMI777xzyPLt27ezcOFC7r//fi688EKuu+46li9fzrJly1i6dCkbNmzgpptu4qWXXmLp0qUsWrSIp5566pA2Fi9ezOzZs1m6dClz585l0aJFUe+2iLRzOlOBuGcUAINufqbBZb+/6rRmbbOgoIDFixfzj3/8g3nz5nHJJZdwzz33HFj+2muvcfrpp9OzZ08ALrroIt59990Dyy+44ALMjBEjRnDEEUcwYsQIAI477jjWrl3LBx98wNixY6l97tlll13GggULmDhx4oE2/vGPfzBp0iQ6d+4MwIUXXtisfRERqaWkkkKZmZmMHTuWsWPHMmLECB555JGE69ZeNsvIyDgwXTtfVVWlu99FJCV0+SsBhQU5TSpPxMqVK1m1atWB+aVLlzJw4MAD86eccgrz589n+/btVFVV8ac//alJ7Y8aNYr58+ezZcsWqqureeKJJzj99NMPWWfMmDE89dRT7N+/n927d/O3v/2t2fsjIgI6U0lIS4YNN2TPnj1ce+217Nixg6ysLI455hhmzpzJF7/4RQD69evHd77zHUaNGkXPnj0ZOnQo3bp1S7j9vn37cs899/DZz372QEf9hAmHviPtpJNO4pJLLuGEE06gT58+nHLKKUndRxHpeDrss79WrFjBsGHDUhRRYvbs2UNBQQFVVVVMmjSJr33ta0yaNCklsbSFn5eIJJee/dXOTJs2jZEjR3L88cczePDgQzrZRUTSkS5/pbH77rsv1SGIiDSJzlRERCRplFRERCRpIk0qZjbOzFaa2Wozu7me5QPMbJ6ZLTGzN81sfMyyW8J6K83snETbFBGR1IksqZhZJvAAcC4wHJhsZnUfvXsr8KS7n0jwDvufh3WHh/PHAeOAn5tZZoJtiohIikTZUT8KWO3uawDMbDYwAXg7Zh0HuobT3YAN4fQEYLa7lwPvm9nqsD0SaDP5fjgE9m46vDy/D9y46vDyJCsoKGDPnj2Rb0dEpKWivPzVD1gXM18alsWaBlxuZqXAXODaRuom0mby1ZdQ4pU3g7tTU1OTtPZERFIh1UOKJwMPu/t/m9lpwGNmdnwyGjazqcBUgAEDBsRf+dmb4aNlzdvQb86rv/zIEXDuPfUvC61du5ZzzjmHU089lcWLF3PxxRfz9NNPU15ezqRJk7jjjjsOWf/ll1/mvvvu4+mnnwbgmmuuobi4mClTpjQvdhGRJIvyTGU90D9mvigsi3Ul8CSAuy8E8oDCOHUTaZOwvZnuXuzuxbVP6k1Hq1at4uqrr+b+++9n/fr1vPbaayxdupTFixezYMGCVIcnItIkUZ6pLAKGmNlgggP/pcCX6qzzIXAm8LCZDSNIKpuBOcDjZvYj4ChgCPAaYAm02XSNnFEwLc4zt77a8GPxEzFw4EBGjx7NDTfcwPPPP8+JJ54IBI9oWbVqFWPGjGlR+yIirSmypOLuVWZ2DfAckAnMcvflZjYdKHH3OcD1wINmdh1Bp/0UDx5GttzMniTogK8CvuXu1QD1tRnVPrSG/Px8IOhTueWWW7jqqqsaXDcrK+uQfpeysrLI4xMRaYpI71Nx97nufqy7f8zdZ4Rlt4UJBXd/290/5e4nuPtId38+pu6MsN7H3f3ZeG1GLr9P08qb4ZxzzmHWrFkHRnmtX7+eTZsOHQgwcOBA3n77bcrLy9mxYwcvvvhi0rYvIpIMqe6obxtaYdjw2WefzYoVKzjttOBNkgUFBfz2t7+lT5+Diat///5cfPHFBx4wWXupTEQkXejR95IQ/bxEOh49+l5ERFJKSUVERJKmQyeVjnDpLxn0cxKRRHXYpJKXl8fWrVt1wGyEu7N161by8vJSHYqItAEddvRXUVERpaWlbN68OdWhpL28vDyKiopSHYaItAEdNqlkZ2czePDgVIchItKudNjLXyIiknxKKiIikjRKKiIikjQd4o56M9sNrExxGIXAlhTHAOkRRzrEAOkRRzrEAOkRRzrEAOkRRzrEAPBxd+/SlAodpaN+ZVMfNZBsZlaS6hjSJY50iCFd4kiHGNIljnSIIV3iSIcYauNoah1d/hIRkaRRUhERkaTpKEllZqoDID1igPSIIx1igPSIIx1igPSIIx1igPSIIx1igGbE0SE66kVEpHV0lDMVERFpBUoqIiKSNO06qZjZODNbaWarzezmFMXQ38zmmdnbZrbczP4zFXGEsWSa2RIzezqFMXQ3sz+a2TtmtsLMTktBDNeF/xZvmdkTZtYqj2A2s1lmtsnM3oop62lmL5jZqvC7Rwpi+GH47/Gmmf3FzLpHGUNDccQsu97M3MwKUxGDmV0b/jyWm9kPooyhoTjMbKSZvWpmS82sxMxGRRxDvcepZv1+unu7/ACZwHvA0UAO8AYwPAVx9AVOCqe7AO+mIo5w+98GHgeeTuG/yyPAf4TTOUD3Vt5+P+B9oFM4/yQwpZW2PQY4CXgrpuwHwM3h9M3AvSmI4WwgK5y+N+oYGoojLO8PPAd8ABSm4GfxWeDvQG443ydFvxfPA+eG0+OBlyOOod7jVHN+P9vzmcooYLW7r3H3CmA2MKG1g3D3je7+eji9G1hBcGBrVWZWBJwHPNTa246JoRvBf6BfA7h7hbvvSEEoWUAnM8sCOgMbWmOj7r4A2FaneAJBoiX8ntjaMbj78+5eFc6+CkT+noMGfhYA9wP/F4h8BFEDMXwTuMfdy8N1NqUoDge6htPdiPh3NM5xqsm/n+05qfQD1sXMl5KCg3ksMxsEnAj8KwWb/zHBf9aaFGy71mBgM/Cb8DLcQ2aW35oBuPt64D7gQ2AjsNPdn2/NGOo4wt03htMfAUekMBaArwHPpmLDZjYBWO/ub6Ri+6Fjgc+Y2b/MbL6ZnZKiOP4L+KGZrSP4fb2ltTZc5zjV5N/P9pxU0oqZFQB/Av7L3Xe18rbPBza5++LW3G49sghO83/h7icCewlOqVtNeE14AkGCOwrIN7PLWzOGhnhwjSFlY/zN7LtAFfC7FGy7M/Ad4LbW3nYdWUBPYDRwI/CkmVkK4vgmcJ279weuIzy7j1q841Siv5/tOamsJ7g+W6soLGt1ZpZN8A/1O3f/cwpC+BRwoZmtJbgMeIaZ/TYFcZQCpe5ee6b2R4Ik05o+B7zv7pvdvRL4M/DJVo4h1r/NrC9A+B355Zb6mNkU4HzgsvDg0do+RpDo3wh/T4uA183syFaOoxT4swdeIzizj3TAQAOuIPjdBPgDweX8SDVwnGry72d7TiqLgCFmNtjMcoBLgTmtHUT4V86vgRXu/qPW3j6Au9/i7kXuPojg5/CSu7f6X+fu/hGwzsw+HhadCbzdymF8CIw2s87hv82ZBNePU2UOwQGE8PuvrR2AmY0juDR6obvva+3tA7j7Mnfv4+6Dwt/TUoKO449aOZSnCDrrMbNjCQaTpOJpwRuA08PpM4BVUW4sznGq6b+fUY9sSOWHYNTEuwSjwL6bohg+TXDK+CawNPyMT+HPZCypHf01EigJfx5PAT1SEMMdwDvAW8BjhCN9WmG7TxD041QSHDSvBHoBLxIcNP4O9ExBDKsJ+h9rfz9/mYqfRZ3la4l+9Fd9P4sc4Lfh78brwBkp+r34NLCYYNTqv4CTI46h3uNUc34/9ZgWERFJmvZ8+UtERFqZkoqIiCSNkoqIiCSNkoqIiCSNkoqIiCSNkopIEphZdfhE2dpP0p4UYGaD6nuar0g6ykp1ACLtxH53H5nqIERSTWcqIhEys7Vm9gMzW2Zmr5nZMWH5IDN7KXyHyYtmNiAsPyJ8p8kb4af2ETKZZvZg+K6L582sU8p2SiQOJRWR5OhU5/LXJTHLdrr7COBnBE+LBvgp8Ii7f4LgAY4/Cct/Asx39xMInou2PCwfAjzg7scBO4AvRLo3Is2kO+pFksDM9rh7QT3lawke9bEmfGDfR+7ey8y2AH3dvTIs3+juhWa2GSjy8H0eYRuDgBfcfUg4fxOQ7e53tcKuiTSJzlREoucNTDdFecx0NeoPlTSlpCISvUtivheG0/8keGI0wGXAP8LpFwnepYGZZYZvyxRpM/TXjkhydDKzpTHz/+vutcOKe5jZmwRnG5PDsmsJ3oB5I8HbML8alv8nMNPMriQ4I/kmwRNsRdoE9amIRCjsUyl291S8k0Ok1enyl4iIJI3OVEREJGl0piIiIkmjpCIiIkmjpCIiIkmjpCIiIkmjpCIiIknz/wHeh2XnedwXiAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sig_loss2, sig_acc2],\n",
    "                   'relu': [relu_loss2, relu_acc2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d50892c7193ec8d92aba2866d7fd9025d172bc37f5538f4292389db9e9b4aec7"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
